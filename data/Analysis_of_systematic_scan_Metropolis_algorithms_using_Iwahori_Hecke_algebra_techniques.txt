                                                                    Analysis of systematic scan Metropolis algorithms
                                                                         using Iwahori-Hecke algebra techniques
arXiv:math/0401318v1 [math.RT] 23 Jan 2004




                                                                                            Persi Diaconisâˆ—
                                                                                       Department of Mathematics
                                                                                          Stanford University
                                                                                          Stanford, CA 94305

                                                                                              Arun Ramâˆ—âˆ—
                                                                                      Department of Mathematics
                                                                                    University of Wisconsin, Madison
                                                                                          Madison, WI 53706
                                                                                          ram@math.wisc.edu

                                             Abstract. We give the first analysis of a systematic scan version of the Metropolis algorithm.
                                             Our examples include generating random elements of a Coxeter group with probability determined
                                             by the length function. The analysis is based on interpreting Metropolis walks in terms of the
                                             multiplication in the Iwahori-Hecke algebra.

                                             1. Introduction
                                                   When faced with a complex task, is it better to be systematic or proceed by making random
                                             adjustments? We study aspects of this problem in the context of generating random elements of a
                                             finite group. For example, suppose we want to fill n empty spaces with zeros and ones such that
                                             the probability of configuration x = (x1 , . . . , xn ) is Î¸ nâˆ’|x| (1 âˆ’ Î¸)|x| with |x| the number of ones in
                                             x. A systematic scan approach works left to right filling each successive place with a Î¸-coin toss.
                                             A random scan approach picks places at random and a given site may be hit many times before all
                                             sites are hit. The systematic approach takes order n steps and the random approach takes order
                                             1
                                             4 n log n steps.
                                                   Realistic versions of this toy problem arise in image analysis and Ising like simulations where
                                             one must generate a random array by a Monte Carlo Markov chain. Systematic updating and ran-
                                             dom updating are competing algorithms discussed in detail in Section 2. There are some successful
                                             analyses for random scan algorithms, but the intuitively appealing systematic scan algorithms have
                                             resisted analysis.
                                                   Our main results show that the binary problem above is exceptional; for the examples analyzed
                                             in this paper, systematic and random scans converge in about the same number of steps.
                                                   Let W be a finite Coxeter group generated by simple reflections s1 , s2 , . . . , sn , where s2i = id.
                                             For example, W may be the permutation group Sn+1 with si = (i, i + 1). The length function â„“(w)
                                             is the smallest k such that w = si1 si2 Â· Â· Â· sik . Fix 0 < Î¸ â‰¤ 1 and define a probability distribution
                                             on W by
                                                                                Î¸ âˆ’â„“(w)                          âˆ’1
                                                                                                                        X
                                                                     Ï€(w) =               ,     where     P   (Î¸    ) =      Î¸ âˆ’â„“(w)                (1.1)
                                                                               PW (Î¸ âˆ’1 )                   W
                                                                                                                          wâˆˆW

                                                  âˆ—
                                                      Research supported in part by National Science Foundation grant DMS-9504379.
                                                  âˆ—âˆ—
                                                       Research supported in part by National Science Foundation grant DMS-9622985.
2                                           persi diaconis                and       arun ram

is the normalizing constant. Thus Ï€(w) is smallest when w = id and, as Î¸ â†’ 1, Ï€ tends to the
uniform distribution. These non-uniform distributions arise in statistical work as Mallows models.
Background and references are in Section 2e.
      A standard Monte Carlo Markov chain algorithm for sampling from Ï€ is the Metropolis algo-
rithm with a systematic scan. This algorithm cycles through the generators in order. If multiplying
by the current generator increases length this multiplication is made. If the length decreases, then
the multiplication is made with probability Î¸ and omitted with probability 1 âˆ’ Î¸. One scan uses
s1 , s2 , . . . , snâˆ’1 , sn , sn , snâˆ’1 , . . . , s1 , in order. Define

                K(w, wâ€² ) = the chance that a systematic scan started at w ends in wâ€² .                                                   (1.2)

Repeated scans of the algorithm are defined by
                                      X
                       K â„“ (w, wâ€² ) =   K â„“âˆ’1 (w, wâ€²â€² )K(wâ€²â€² , wâ€² ),                                   â„“ â‰¥ 2.                             (1.3)
                                                     w â€²â€²

In Section 2c and 4a we show that this Markov chain has Ï€ as unique stationary distribution.
    The main results of this paper derive sharp results on rates of convergence for these walks.
As an example of what our methods give, we show that order n scans are necessary and suffice to
reach stationary on the symmetric group starting from the identity. More precisely, we prove

Theorem 1.4. Let Sn be the permutation group on n letters. Fix Î¸, 0 < Î¸ â‰¤ 1. Let
K1â„“ (w) = K â„“ (id, w) be the systematic scan chain on Sn defined by (1.2) and (1.3). For â„“ =
n/2 âˆ’ (log n)/(log Î¸) + c with c > 0,
                                     2c+1           2
                   kK1â„“ âˆ’ Ï€Î¸ k2T V â‰¤ eÎ¸    âˆ’ 1 + n!Î¸ n /8âˆ’n(log n)/(log Î¸)+n(c+1/4) .   (1.5)

Conversely, if â„“ â‰¤ n/4 then, for fixed Î¸, kK1â„“ âˆ’ Ï€kT V tends to 1 as n â†’ âˆž.

     The total variation norm above is defined in Section 2a below. Note that the upper bound
in (1.5) tends to zero for c large, so that about n/2 scans suffice to reach stationarity. The lower
bound shows this is of the right order for large n.
     Each scan above uses 2n multiplications. Thus Theorem 1.4 implies that the systematic scan
approach reaches stationarity in n2 operations up to lower order terms. We also conjecture that
the random scan approach (see Section 2b) for this example takes order n2 operations. Further,
in Section 7, we prove that the scan based on the sequence

          (s1 , s2 , . . . , sn , sn , . . . , s1 ), (s1 , . . . , snâˆ’1 , snâˆ’1 , . . . , s1 ), . . . , (s1 , s2 , s2 , s1 ), (s1 , s1 )

converges in one pass. Thus, again, up to lower order terms, n2 operations suffice to reach sta-
tionarity. These results shows that various different scanning strategies take the same number of
operations to reach stationarity.
     One novel aspect of present arguments is our use of the Iwahori-Hecke algebra H spanned by
the symbols {Tw }wâˆˆW . This is generated by Ti = Tsi , 1 â‰¤ i â‰¤ n with the relations
                                  
                                    Tsi w ,              if â„“(si w) > â„“(w),
                         Ti Tw =
                                    qTsi w + (q âˆ’ 1)Tw , if â„“(si w) < â„“(w).

We have succeeded in giving an algebraic interpretation of the Markov chain K(w, wâ€² ) as multipli-
cation in the Iwahori-Hecke algebra H. From there, knowledge of the center of H (via a result of
                          Metropolis scans and Hecke algebras                                       3

Brieskorn-Saito and Deligne) allows us to explicitly diagonalize K(w, wâ€² ). Convergence bounds are
given in terms of the eigenvalues and the generic degrees of representation theory. Then calculus
leads to results like Theorem 1.4.
     Section 2 collects together probabilistic background and tools. We explain Markov chains, the
Metropolis algorithm, systematic scans, and relate the basic Metropolis chain to a natural walk
on the chambers of a building. In Section 2e we develop properties of the measures Ï€. Some of
these are new even for reflection groups of type A (the symmetric group). These properties will
be applied to prove lower bounds for walks as in Theorem 1.4.
     Section 3 collects together representation theoretic background and tools and connects the
representation theory to Markov chains. Section 4 connects Hecke algebras to the Metropolis
algorithm and specializes the results from Section 3. A basic upper bound for convergence is
derived by relating two inner products.
     Sections 5 and 6 derive results for the hypercube and the dihedral groups. Here we find that
both the systematic and random scans converge in about the same number of steps â€“ the differences
are only in the lead term constants (which are functions of Î¸).
     Section 7 derives results for two different systematic scanning plans for the symmetric group.
Though we do not have the space to treat further examples in this paper, it should be remarked
that the methods of Section 7 should also produce analogous results for the Weyl groups of type
Bn and the imprimitive complex reflection groups G(r, 1, n). The long and short systematic scans
can be defined in a similar way and the representation theory goes through without problems (see
[Hf], [Ra] and [AK]). The remaining necessary ingredient is an analogue of Lemma 7.2.
Acknowledgement. This paper is dedicated to our friend Bill Fulton. We are also thankful to Ruth
Lawrence for early efforts to help understand deformed random walks.

2. Probabilistic background
     In this section we give background material for Markov chains, the Metropolis algorithm, and
systematic scans. In Section 2d we interpret the basic walk as a walk on flags and the chambers
of a building and in Section 2e we derive basic properties of the stationary distributions.

2a. Markov chains
    Background for Markov chains may be found in any standard probability text e.g. Feller [Fe,
Chapter XV]. For the quantitative theory developed here see Saloff-Coste [SC] and the references
therein.
    Let X be a finite set. A Markov chain on X is a matrix K = (K(x, y))x,yâˆˆX such that
                                                             X
                           K(x, y) âˆˆ [0, 1]       and              K(x, y) = 1.
                                                             yâˆˆX


The set X is the state space and K(x, y) gives the probability of moving from x to y in one step.
Powers of the matrix K give the probability of moving from x to y in more steps. For example,
                                                  X
                                   K 2 (x, y) =         K(x, z)K(z, y)
                                                  zâˆˆX


indicates that to move from x to y in two steps the chain must move to z and then from z to y.
The chain is irreducible and aperiodic if there is an â„“ > 0 such that K â„“ (x, y) > 0 for all x, y âˆˆ X.
4                                persi diaconis            and   arun ram
                                                                                              X
The chain K is reversible if there is a stationary distribution Ï€: X â†’ [0, 1],                      Ï€(x) = 1, such
                                                                                              xâˆˆX
that, for all x, y âˆˆ X,
                                     Ï€(x)K(x, y) = Ï€(y)K(y, x).
For irreducible aperiodic K, reversibility implies that, for each x âˆˆ X, the real numbers K â„“ (x, y)
converge to Ï€(y) as â„“ â†’ âˆž.
     The quantitative theory of Markov chains studies the speed of convergence. The total variation
distance of K â„“ (x, Â·) to Ï€ is defined by
                                                                      
                                                                      
                                                   X                  
                                   â„“                     â„“
                                kKx âˆ’ Ï€kT V = max 
                                                      K (x, y) âˆ’ Ï€(y) .
                                              AâŠ†X                     
                                                   yâˆˆA


Using the set A = {y âˆˆ X | K â„“ (x, y) > Ï€(y)} it is easily shown that

                                                    1 X â„“
                               kKxâ„“ âˆ’ Ï€kT V =          |K (x, y) âˆ’ Ï€(y)|.                                    (2.1)
                                                    2
                                                     yâˆˆX


Let L2 (Ï€) be the space of functions f : X â†’ R with the norm
                                                    X
                                     hf, gi2 =            f (x)g(x)Ï€(x).                                     (2.2)
                                                    xâˆˆX


The following lemma provides a relation between the total variation and the L2 (Ï€) norms. This
bound is the primary tool for studying rates of convergence of Markov chains.

Lemma 2.3. Let f âˆˆ L2 (Ï€). Then kf k2T V â‰¤ 14 kf /Ï€k22 .

Proof. By the Cauchy-Schwartz inequality,
                                       !2                        !                !
                     X |f (x)| p                     X f (x)2        X
        2        1                              1
     kf kT V =   4
                         p      Ï€(x)        â‰¤   4                          Ï€(x)       = 14 hf /Ï€, f /Ï€i2 .
                           Ï€(x)                        Ï€(x)
                     xâˆˆX                            xâˆˆX              xâˆˆX



2b. Systematic scan algorithms
     Let Ï€ be a probability distribution on a finite set X and let K1 , K2 , . . . , Kn be Markov chains
on X each having stationary distribution Ï€. Then any product Kiâ„“ Kiâ„“âˆ’1 Â· Â· Â· Ki1 has stationary
distribution Ï€ and a choice of an infinite sequence {iâ„“ }âˆž  â„“=1 gives a scanning strategy. A ran-
dom choice of indices gives a random scanning stategy. If each Ki is reversible for Ï€, then
K1 K2 Â· Â· Â· Knâˆ’1 Kn Kn Knâˆ’1 Â· Â· Â· K2 K1 is an example of a reversible systematic scanning strategy
(while K1 Â· Â· Â· Kn is not necessarily reversible).
     In routine applications of the Metropolis algorithm to image analysis and Ising-like models the
state space has coordinates. Randomized strategies choose a coordinate at random and attempt
to change it. Systematic strategies cycle through the coordinates in various orders. Fishman [Fi]
reviews the literature on scanning strategies and gives some practical comparison. The scheme
underlying Theorem 1.4 is Fishmanâ€™s Plan 3.
     There has been some rigorous work on rates of convergence for systematic scans in a related
case: Gaussian distribution of coordinates with the stochastic updating done by the heat bath
                          Metropolis scans and Hecke algebras                                     5

algorithm (also known as Glauber dynamics or the Gibbs sampler). One fascinating study by
Goodman and Sokal [GS] relates scanning strategies to standard approaches for solving large
linear systems. They show that the systematic scan heat bath algorithm is a stochastic analog of
the Gauss-Seidel algorithm. Moreover, they show how previous analyses of Gauss-Seidel give the
eigenvalues of its stochastic counterpart. Amit [Am1-2] and Amit and Grenander [AG] have pushed
forward and carried out these ideas to give some comparison of systematic and randomized sweeps
in the Gaussian case. Their approach uses the fact that the heat bath algorithm is a projection
operator. In the Gaussian case the problem reduces to the computation of angles between subspaces
of a Hilbert space. Baronne and Frigessi [BF] and Roberts and Sahk [RS] are related references.

2c. The Metropolis algorithm
    The Metropolis algorithm gives a way of changing the stationary distribution of a given Markov
chain into any distribution. It was invented by Metropolis et al [MR]. A clear description is in
Hammersley and Handscomb [HH] and a recent survey appears in [DS].
    Let X be a finite set. Let P (x, y) = P (y, x) be a symmetric Markov matrix on X and let Ï€
be a fixed probability distribution on X. Form a new chain by the following recipe:
                  ï£±
                  ï£´
                  ï£´ P (x, y),                                if x 6= y and Ï€(y) â‰¥ Ï€(x),
                  ï£´
                  ï£´
                  ï£´
                  ï£´
                  ï£´
                  ï£²          Ï€(y)
                    P (x, y)      ,                          if x 6= y and Ï€(y) < Ï€(x),
       M (x, y) =            Ï€(x)                                                             (2.4)
                  ï£´
                  ï£´                                     
                  ï£´
                  ï£´                X                Ï€(z)
                  ï£´
                  ï£´ P (x, x) +         P (x, z) 1 âˆ’       , if x = y.
                  ï£´
                  ï£³                                 Ï€(x)
                               Ï€(z)<Ï€(x)


In words:
     Form the Metropolis chain from x by choosing y from P (x, y). If Ï€(y) â‰¥ Ï€(x) move to
     x. If Ï€(y) < Ï€(x) flip a coin with chance of heads Ï€(y)/Ï€(x). If the coin comes up heads
     move to y. In all other cases stay at x.
As shown in the references above, the Metropolis chain is reversible with stationary distribution
Ï€. It is of practical importance that the chain M can be run knowing Ï€ only up to a normalizing
constant. Irreducibility and aperiodicity of M must be checked on a case by case basis.
     An example of interest is X = W , where W is a finite real reflection group generated by simple
reflections s1 , s2 , . . . , sn . Let P (x, y) be the Markov chain given by
                                                 
                                                    1/n, if y = si x for some i,
                                      P (x, y) =
                                                    0,   otherwise.

Here P (x, y) is the usual random walk based on         a generating set. It has uniform stationary
distribution. Fix Î¸, 0 < Î¸ â‰¤ 1 and let Ï€ be as in       (1.1). The Metropolis construction gives the
Markov chain
                            ï£±
                            ï£´ 1/n,                      if y = si x and â„“(y) > â„“(x),
                            ï£´
                            ï£´
                            ï£´
                            ï£´
                            ï£´
                            ï£² Î¸/n,                      if y = si x and â„“(y) < â„“(x),
                            ï£´
                            ï£´
                  M (x, y) = 1        X                                                        (2.5)
                            ï£´
                            ï£´                (1 âˆ’ Î¸),   if y = x,
                            ï£´
                            ï£´
                            ï£´
                            ï£´  n
                            ï£´    â„“(si x)<â„“(x)
                            ï£´
                            ï£³
                              0,                        otherwise,
6                                    persi diaconis      and     arun ram

which has stationary distribution Ï€. In Section 4a we demonstrate that this is exactly the chain
given by left multiplication by a uniformly chosen generator TÌƒi in the Iwahori-Hecke algebra H with
q = Î¸ âˆ’1 . Similarly, the systematic scan chain of Theorem 1.4 can be interpreted via multiplication
in H.
     Despite its widespread use there has been very limited success in analyzing the time to sta-
tionarity of the Metropolis algorithm. In the present paper we carry this out for the random scan
Metropolis algorithm (2.5) on the hypercube (Section 5) and on the dihedral group (Section 6).
Though we have not analyzed the random scan Metropolis algorithm on the symmetric group we
conjecture that order n2 steps are necessary and sufficient to achieve stationarity. A survey of
what is rigorously known appears in [DS].
     Diaconis and Hanlon [DH] studied the example given by W = Sn , the symmetric group, (so
c(w) = n âˆ’ [# of cycles in w]), with input chain
                                 ï£± 1
                                 ï£² ,      if y = (i, j)x for some transposition (i, j),
                                   n
                    P (x, y) =        2                                                                    (2.6)
                                 ï£³
                                     0,    otherwise,

and stationary distribution Ï€(w) = zÎ¸ c(w), where z is a normalizing constant and c(w) is the
minimum number of transpositions needed to sort w. They showed that all eigenvectors of the
resulting Metropolis chain are given by the coefficients of Jackâ€™s symmetric functions (expanded in
terms of the power sum symmetric functions) and they used the corresponding eigenvalues to give
a complete analysis of the running time.
     Similar analyses were carried out in the Ph.D. theses of Belsley [Be2] and Silver [Si]. They
worked in abelian groups with Ï€ proportional to Î¸ â„“(y) where â„“ is the length function with respect
to a natural set of generators. In several cases they found that the eigenfunctions were natural
deformations of classical orthogonal polynomials. Ross and Xu [RX] studied the random scan
Metropolis algorithm on the hypercube using its representation as a random walk on a hypergroup.
It should be emphasized that for other choices of Ï€, or in non-group cases, careful analysis of rates
of convergence for the Metropolis algorithm is completely open.

2d. Some other interpretations of the walks
     We have presented Theorem 1.4 in an algorithmic context. Here we show how the walk (2.5)
arises geometrically on the space of flags and as the natural nearest neighbor walk on the chambers
of a building. The systematic scan walks have similar interpretations.
     Let Fq be a finite field. A complete flag F = (0 = F0 âŠ† F1 âŠ† F2 âŠ† Â· Â· Â· âŠ† Fnâˆ’1 âŠ† Fn = V )
is a nested increasing sequence of subspaces of an n-dimensional vector space V over Fq with
dim(Fi ) = i. A natural random walk on complete flags may be performed as follows:
          Choose i, 1 â‰¤ i â‰¤ n âˆ’ 1, uniformly.
          Replace Fi by a uniformly chosen subspace FÌƒi with Fiâˆ’1 âŠ† FÌƒi âŠ† Fi+1 .
This walk is symmetric, irreducible and aperiodic. It thus has the uniform distribution as its
unique stationary measure. It is instructive to think of the â€œq = 1â€ case. Then a flag is a nested
increasing chain {i1 } âŠ† {i1 , i2 } âŠ† Â· Â· Â· âŠ† {i1 , i2 , . . . , in } of elements of an n set or, equivalently, a
permutation (i1 , i2 , . . . , in ). In this case the walk is multiplication by random pairwise adjacent
transpositions.
     The space of flags may also be identified as the chambers of a building of type Anâˆ’1 and in
this formulation the walk is described as follows:
          From a chamber C of the building choose one of the adjacent chambers uniformly at
          random and move there.
                           Metropolis scans and Hecke algebras                                     7

In the elegant, readable treatment of buildings by Brown [Bw], he explains that flag space may
be represented as G/B with G = GLn (Fq ) and B the subgroup of upper triangular matrices in
GLn (Fq ). Then two flags g1 B and g2 B differ in the ith step as above if and only if g1 Pi = g2 Pi
where Pi is the parabolic subgroup Pi = B âˆª Bsi B (see [Bw, pp. 102-103]). Thus, if flags g1 B
and g2 B are i-adjacent then g2 = g1 b or g2 = g1 bsi bâ€² with b, bâ€² âˆˆ B, and so the walk on G/B
moves from gB to gg â€² B with g â€² uniformly chosen in B or Bsi B. In this way, choosing an adjacent
chamber of the building at random produces a B-invariant walk on G/B. Finally, the walk on flags
gives rise to a natural walk on the double coset space B\G/B (described in more detail in Section
3b). The double coset space is identifiable with the symmetric group Sn and the induced Markov
chain is given by (2.5) with Î¸ = 1/q. A similar story holds for the natural walk on any spherical
building.

2e. Properties of the stationary distribution
    Suppose that (X, d) is a finite metric space. A simple way of building probability models on
X is to fix 0 < Î¸ â‰¤ 1 and x0 âˆˆ X and define

                         q d(x,x0 )                                     X
                Ï€(x) =              ,   where q = Î¸ âˆ’1   and PX (q) =         q d(x,x0 )        (2.7)
                          PX (q)
                                                                        xâˆˆX

is a normalizing constant. When q = 1 the distribution is uniform.
     Models of the form (2.7) were introduced by Mallows [Ma] for the study of permutations. He
used the length function as a distance, â„“(xâˆ’1 x0 ) = d(x, x0 ), and estimated q and x0 to match data.
Such Mallows models have had application and development for ranked and partially ranked data
using a variety of metrics [D], [Cr], [FV], [Mar]. They have also been used for phylogenetic trees
[BHV], classification trees [SB] and compositions [Det].
     One problem in studying Mallows models is that the normalizing constant PX (q) is uncom-
putable in general. In such cases properties of Ï€ can be studied by simulation using the Metropolis
algorithm given in Section 2c.
     For the examples based on reflection groups the normalizing constants are known and further
there is a simple algorithm available for exact generation from Ï€. These properties are collected
together here. In each case the properties are illustrated for the permutation group; some of our
results are new for the original Mallows model. Further, the properties of Ï€ (particularly Property
4 below) are used in proving the lower bounds in Theorem 1.4.
     Throughout this section we work with the model where the underlying space X = W is a
finite Coxeter group generated by simple reflections, x0 is the identity element of W , and the
length function is the distance on W . Thus the model is

                          q â„“(w)                                         X
                 Ï€(w) =          ,      where q = Î¸ âˆ’1   and PW (q) =           q â„“(w)          (2.8)
                          PW (q)
                                                                         wâˆˆW

is the PoincareÌ polynomial of the group W . It is a classical theorem that the normalizing constant
has a simple form:
                                                   n
                                                  Y   q di âˆ’ 1
                                       PW (q) =                ,                               (2.9)
                                                  i=1
                                                       qâˆ’1

for known integers di , the degrees of W (see [Hu, Theorem 3.15]). For the symmetric group Sn+1 ,
di = i + 1 for 1 â‰¤ i â‰¤ n. The PoincareÌ polynomial PW (q) will be used crucially in what follows.
Property 1. Ï€(w) = Ï€(wâˆ’1 ), since â„“(w) = â„“(wâˆ’1 ).
8                                  persi diaconis       and      arun ram

This invariance under inversion was first used by Mallows [Ma] to characterize Mallows models
in a larger class of measures as follows: Suppose n objects are to be ranked by making pairwise
comparisons. Suppose the true ranking is 1 < 2 < 3 < Â· Â· Â· < n and a subject ranks objects i and j
correctly with probability pij . Let Q(w) be the chance that the comparisons
                                                                           lead to the permutation
w given that they are all consistent. Of course, Q(w) depends on the n2 parameters pij . Mallows
                               âˆ’1                                                     â„“(w) r(w)
proved that
         P if Q(w) = Q(w ), then for some real numbers q and Ï†, Q(w) = zq                 Ï†     with
r(w) =      iw(i) and z a normalizing constant. He further showed that the two parameters q
and Ï† were practically indistinguishable for large n and suggested setting Ï† = 1, leading to the
distribution Ï€(w).
Property 2. Let J âŠ† {1, 2, . . . , n} and let WJ be the subgroup of W generated by the the
generators si for i âˆˆ J . The group WJ is a parabolic subgroup of W . Each coset of WJ in W
contains a unique coset representative xj of minimal length [Hu, Prop. 1.10] and the probability
of any such coset is computable via

                                                               PWJ (q)
                                        Ï€(xj WJ ) = q â„“(xj )             .                           (2.10)
                                                               PW (q)


As an example suppose that W is the symmetric group Sn generated by s1 , s2 , . . . , snâˆ’1 where
si = (i, i + 1). If J = {1, 2, . . . , n âˆ’ 2} then WJ is the subgroup of permutations which leave n
fixed. The minimal length coset representatives xj for the cosets of WJ in W have j in position n
and the rest of the entries in order. Property 2 says that

                                                                         (1 âˆ’ q)
                               Ï€ ({w âˆˆ Sn | w(n) = j}) = q nâˆ’j                     .                 (2.11)
                                                                        (1 âˆ’ q n )

Similarly, if J = {2, 3, . . . , n âˆ’ 1} Property 2 yields

                                                                      (1 âˆ’ q)
                                Ï€ ({w âˆˆ Sn | w(1) = j}) = q jâˆ’1                 .                    (2.12)
                                                                     (1 âˆ’ q n )

Similar formulas can be derived for the cases where J consists of the first j or last j elements of
{1, 2, . . . , n}.
     In combination with Property 1, (2.11) also provides a fomula for the probability of the set of
permutations with j in the nth position and (2.12) gives the probability of the set of permutations
with j in the first position. More generally, one can give formulas for the probability of the set of
permutations which have 1, 2, . . . , j in any given relative position.
Property 3. Let J1 âŠ‡ J2 âŠ‡ Â· Â· Â· âŠ‡ Jk = âˆ… be a sequence of subsets of {1, 2, . . . , n}. Then a
sequential algorithm for generating w in W from Ï€ is to choose, for each 1 â‰¤ i â‰¤ k âˆ’ 1, the minimal
length coset representative of a coset of WJi+1 in WJi , 1 â‰¤ i â‰¤ k âˆ’ 1 and multiply these together.
If xi is a minimal length coset representative of a coset in WJi /WJi+1 choose xi with probability
q â„“(xi ) PWJi+1 (q)/PWJi (q).
    As an example, suppose W is the symmetric group Sn generated by s1 , s2 , . . . , snâˆ’1 . If J1 âŠ‡
J2 âŠ‡ Â· Â· Â· âŠ‡ Jnâˆ’1 is given by Ji = {i, i + 1, . . . , n âˆ’ 1} the algorithm can be realized as the following
sequential procedure: Place symbols down sequentially beginning with 1. If symbols 1, 2, . . . , i âˆ’ 1
have been placed in some order, place i first with probability q iâˆ’1 (1 âˆ’ q)/(1 âˆ’ q i ), second with
probability q iâˆ’2 (1 âˆ’ q)/(1 âˆ’ q i ), . . ., ith with probability (1 âˆ’ q)/(1 âˆ’ q i ). Continuing until all n
elements are placed gives an efficient method of choosing from Ï€.
                             Metropolis scans and Hecke algebras                                        9

    An application of this is the following clever algorithm suggested by Pak [Pa] for generating
a uniformly chosen element of GLn (Fq ). Choose w âˆˆ Sn with probability proportional to q â„“(w) .
Then form b1 wb2 with b1 and b2 uniformly chosen in the lower triangular matrices in GLn (Fq ).
This yields an efficient algorithm for uniform choice in GLn (Fq ). With obvious modifications this
procedure easily adapts to the other finite groups with a BN pair.

Property 4. Consider a finite Coxeter group with probability distibution Ï€ as given in (2.8). Let
Z be the random variable given by Z(w) = â„“(w) for w âˆˆ W . Then, with di as in (2.9),

                            n
                       X di q di                                                  n
                                                                               X d2 q di
                 nq                                                   nq               i
       EÏ€ (Z) =      âˆ’             ,            and     VarÏ€ (Z) =           âˆ’                  .   (2.13)
                1 âˆ’ q i=1 1 âˆ’ q di                                 (1 âˆ’ q) 2
                                                                               i=1
                                                                                   (1 âˆ’ q di )2



Proof. The moment generating function of Z is

                                                                 n
                                   1    X             P (et q) Y (1 âˆ’ (et q)di ) (1 âˆ’ q)
         MZ (t) = EÏ€ (etZ ) =             (et q)â„“(w) = W      =                            .
                                 PW (q)                PW (q)   i=1
                                                                    (1 âˆ’ et q) (1 âˆ’ q di )
                                        wâˆˆW


It follows that Z is the sum of independent random variables Z1 , . . . , Zn , where

                                                 (1 âˆ’ (et q)di ) (1 âˆ’ q)
                                     MZi (t) =                            .
                                                   (1 âˆ’ et q) (1 âˆ’ q di )

Then
                                             d                q     di q di
                                EÏ€ (Zi ) =      MZi (t)t=0 =     âˆ’
                                             dt               1âˆ’q   1 âˆ’ q di
and
                          d2                   q        2di q di +1      2q 2      d2i q di
            EÏ€ (Zi2 ) =       M Z   (t)     =     âˆ’                   +         âˆ’           .
                          dt2     i      t=0   1âˆ’q   (1 âˆ’ q)(1 âˆ’ q di ) (1 âˆ’ q)2   1 âˆ’ q di
It follows that
                                                   q         d2i q di
                                 VarÏ€ (Zi ) =            âˆ’              .
                                                (1 âˆ’ q)2   (1 âˆ’ q di )2

     We remark that for Coxeter groups of type An , Bn , Dn under the probability distribution Ï€,
â„“(w) has an approximately normal distribution with mean and variance as in (2.13). This follows
from its representation as a sum of independent variables in the proof of Property 4. For details,
see [D, Ch. 6C, Cor. 1-2].

3. Hecke algebras

    This section introduces Hecke algebras as bi-invariant functions on a group. We develop
the needed Fourier analysis and then specialize to the Iwahori-Hecke algebras associated to finite
Coxeter groups.

3a. Algebras and Fourier analysis
10                                 persi diaconis        and   arun ram

    Random walks are traditionally analyzed using Fourier analysis [D]. We find this possible in
our examples and here explain the basic tools.
    An algebra H over C is (split) semisimple if it is isomorphic to a direct sum of matrix algebras.
This means that there exists a finite index set WÌ‚ , and positive integers dÎ» , Î» âˆˆ WÌ‚ , such that
                                              M
                                         Hâˆ¼=       MdÎ» (C),
                                                 Î»âˆˆWÌ‚

where MdÎ» (C) is the algebra of dÎ» Ã— dÎ» matrices with entries in C. Fix an isomorphism
           M
  Ï†: H âˆ’â†’      MdÎ» (C)       and define     eÎ»ST = Ï†âˆ’1 (EST
                                                         Î»
                                                            ),     Î» âˆˆ WÌ‚ , 1 â‰¤ S, T â‰¤ dÎ» ,           (3.1)
              Î»

         Î»
                              L
where EST   is the matrix in    Î» MdÎ» (C) which has a 1 in the (S, T ) entry of the Î»th block and
zeros everywhere else. The elements eÎ»ST âˆˆ H are a set of matrix units for H.
     The matrix units {eÎ»ST } form a basis of H and we write
                                       X       X
                                  h=                ÏÎ»ST (h)eÎ»ST ,                           (3.2)
                                          Î»âˆˆWÌ‚ 1â‰¤S,T â‰¤dÎ»

for h âˆˆ H. The homomorphisms ÏÎ» : H â†’ MdÎ» (C) and the linear functionals Ï‡Î»H : H â†’ C given by
                                   
                  ÏÎ» (h) = ÏÎ»ST (h) 1â‰¤S,T â‰¤d  and     Ï‡Î»H (h) = Tr(ÏÎ» (h))
                                                 Î»


 are the irreducible representations and the irreducible characters of H, respectively. The homo-
 morphisms ÏÎ» depend on the choice of Ï† but the irreducible characters Ï‡Î»H do not.
      A trace on H is a linear functional ~t: H â†’ C such that ~t(h1 h2 ) = ~t(h2 h1 ), for all h1 , h2 âˆˆ H.
 Up to constant multiples there is a unique trace on MdÎ» (C) and this implies that for any trace
~t: H â†’ C on H there are unique tÎ» âˆˆ C, Î» âˆˆ WÌ‚ , such that
                                                  X
                                             ~t =   tÎ» Ï‡Î»H .                                           (3.3)
                                                  Î»âˆˆWÌ‚

The trace ~t is nondegenerate if tÎ» 6= 0 for all Î» âˆˆ WÌ‚ . Define a symmetric bilinear form h, iH : HÃ—H â†’
C on H by
                                hh1 , h2 iH = ~t(h1 h2 ),    for h1 , h2 âˆˆ H.
The form h, iH is nondegenerate if and only if ~t is a nondegenerateP
                                                                    trace.
     Let {Tw }wâˆˆW be a basis of H. The Fourier transform of h = wâˆˆW hw Tw at the representa-
tion Ï is                                      X
                                      hÌ‚(Ï) =        hw Ï(Tw ).                        (3.4)
                                                  wâˆˆW

The Fourier inversion theorems describe the change of basis matrix between {Tw } and {eÎ»ST } and
recovers h from {hÌ‚(ÏÎ» )}Î»âˆˆWÌ‚ .

Theorem 3.5. (Fourier inversion and Plancherel) Let H be a semisimple algebra over C with a
nodegenerate trace ~t. Let {Tw }wâˆˆW be a basis for the algebra H. Let {TÌƒwâˆ— }wâˆˆW be the dual basis
with respect to h, iH , that is hTÌƒwâˆ— , Tv iH = Î´wv . Then, with notations as in (3.1-3.4),
                                                 X
                                          eÎ»ST =       tÎ» ÏÎ»T S (TÌƒwâˆ— )Tw ,
                                               wâˆˆW
                                  Metropolis scans and Hecke algebras                                                         11

and, for any h, h1 , h2 âˆˆ H,
                                         X
                                  hw =          tÎ» Tr(hÌ‚(ÏÎ» )ÏÎ» (TÌƒwâˆ— )),        for h âˆˆ H, and                          (3.6)
                                         Î»âˆˆWÌ‚

                                            X                            
                        hh1 , h2 iH =             tÎ» Tr hÌ‚1 (ÏÎ» )hÌ‚2 (ÏÎ» ) ,          for h1 , h2 âˆˆ H.                   (3.7)
                                           Î»âˆˆWÌ‚


                                                                            P
Proof. Since ~t is nondegenerate, the equation ~t(eÎ»ST ) =                               Âµ   Î»
                                                                                ÂµâˆˆWÌ‚ tÂµ Ï‡H (eST )     = tÎ» Î´ST implies that
                             
                      eÎ»T S                                        Î» 	
                                    is the dual basis to           eST             with respect to h, iH .
                       tÎ»

                          1
By (3.2), ÏÎ»ST (a) =        ha, eÎ»T S iH . Thus
                         tÎ»
                                         X                       X
                              eÎ»ST =        heÎ»ST , TÌƒwâˆ— iH Tw =   tÎ» ÏÎ»T S (TÌƒwâˆ— )Tw .
                                           wâˆˆW                          wâˆˆW

The equation (3.6) is
                                                         X                         X
               hw = hh, TÌƒwâˆ— iH = ~t(hTÌƒwâˆ— ) =                 tÎ» Ï‡Î»H (hTÌƒwâˆ— ) =          tÎ» Tr(hÌ‚(ÏÎ» )ÏÎ» (TÌƒwâˆ— )),
                                                        Î»âˆˆWÌ‚                       Î»âˆˆWÌ‚

and (3.7) is
                                                  X                         X
               hh1 , h2 iH = ~t(h1 h2 ) =              tÎ» Ï‡Î»H (h1 h2 ) =        tÎ» Tr(hÌ‚1 (ÏÎ» )hÌ‚2 (ÏÎ» )).
                                                   Î»                        Î»



3b. Coset chains and Hecke algebras
     Let G be a finite group, B a subgroup of G and let Q be a left B-invariant probability
distribution on G. Right multiplication by random picks from Q induces a random walk on G,

                                         X0 = x0 , X1 = x0 g1 , X2 = x0 g1 g2 , . . . ,                                  (3.8)

which, in turn, induces a process on B cosets, Y0 , Y1 , Y2 , . . ., where Yi is the coset containing Xi .
The chain on G produced by right multiplication by random picks from Q is KÌƒ(x, y) = Q(xâˆ’1 y).
The chance that this chain winds up in an element of yB is KÌƒ(x, yB) = Q(xâˆ’1 yB) and, since Q is
left B-invariant, KÌƒ(x, yB) = KÌƒ(xb, yB) for any b âˆˆ B. This invariance is a necessary and sufficient
condition for the induced coset process to be a Markov chain for any starting state x0 B âˆˆ G/B
(see Theorem 6.32 of [KS]). If the support of Q is not a coset of a subgroup of G then the chain in
(3.8) is irreducible and aperiodic with uniform stationary distribution. The resulting coset chain
is
             K(xB, yB) = Q(xâˆ’1 yB) with stationary distribution Ï€(xB) = |B|/|G|.
    If the probability Q is B bi-invariant then the right process (3.8) on G induces a process on B
double cosets by simply reporting which double coset the element Xi is in. The chance the G-chain
moves from x to an element of ByB in one step is Q(xâˆ’1 ByB) and since this only depends on the
12                                   persi diaconis          and    arun ram

double coset of x the induced process is a Markov chain on double cosets for any starting state
Bx0 B. Letting W be a set of coset representatives for the double cosets of B in G, the chain is
given by
          K(w, wâ€² ) = K(wâˆ’1 Bwâ€² B),               with stationary distribution Ï€(w) = |BwB|/|G|,
where we view the double coset chain as a Markov chain on the set W .
    The Hecke algebra of the pair (G, B) is the subalgebra of the group algebra of G consisting of
the B bi-invariant functions on G,
                     H = {f : G â†’ C | f (g) = f (b1 gb2 ), for g âˆˆ G, b1 , b2 âˆˆ B}.
Background on Hecke algebras may be found in Curtis and Reiner [CR, Â§11D]. If H is commutative
then (G, B) is called a Gelfand pair and there is a well developed probabilistic literature surveyed
in [Le1-2] or [D, Ch. 3F].
     Let W be a set of representatives of the double cosets in B\G/B. The functions
                                           1
                                    Tw =      Î´BwB ,     w âˆˆ W,
                                          |B|
form a basis of H, where Î´BwB is the characteristic function of the double coset BwB. The
natural anti-involution on G given by g 7â†’ g âˆ’1 induces an anti-involution âˆ— : H â†’ H on H given
by Tw 7â†’ Twâˆ’1 . The trivial representation of G restricts to the index representation of H given by
                                                                   |BwB|
                         Ï1 (Tw ) = ind(w),     where ind(w) =            .                     (3.9)
                                                                     |B|
An example to keep in mind is G = GLn (Fq ) with B the subgroup of upper triangular matrices.
Then W is the set of permutation matrices and ind(w) = q â„“(w) .
     Let L(G/B) = {f : G â†’ C | f (g) = f (gb) for g âˆˆ G, b âˆˆ B}. The group G acts on the left of
L(G/B) and H(G/B) acts on the right of L(G/B) by convolution. The raison dâ€™etre for the Hecke
algebra is that H = EndG (L(G/B)) and, as (G, H) bimodules,
                                                 X
                                     L(G/B) =        GÎ» âŠ— H Î» ,                                (3.10)
                                                          Î»âˆˆWÌ‚

where Î» runs over an index set WÌ‚ of all the irreducible representations of H, GÎ» is an irreducible
G-module and H Î» is an irreducible H module, see [CR, (11.25)(ii)]. Centralizers of the action of
a finite group, in this case G acting on L(G/B), are semisimple (our base field is C) and therefore
theory of Section 3a applies to Hecke algebras. The trace of the action of H on L(G/B) is given
by                                          
                                   ~t(Tw ) = |G|/|B|, if w = 1,                              (3.11)
                                              0,      otherwise.
The decomposition (3.10) yields
                                        X
                               ~t =            tÎ» Ï‡Î»H ,    where tÎ» = dim(GÎ» ),                    (3.12)
                                        Î»âˆˆWÌ‚
      Î»
and Ï‡H are the irreducible characters of H. Define an inner product on H by
                                    hh1 , h2 iH = ~t(h1 h2 ),      h1 , h2 âˆˆ H.
The basis                          
                            Twâˆ’1
                                                     is the dual basis to     {Tw }wâˆˆW             (3.13)
                           ind(w)       wâˆˆW
with respect to h, iH , i.e. h(1/ind(v))Tvâˆ’1 , Tw iH = Î´vw , for all v, w âˆˆ W (see [CR, (11.30)(iii)]).

3c. Iwahori-Hecke algebras
                          Metropolis scans and Hecke algebras                                       13

     The Hecke algebras associated to finite Chevalley groups G and their Borel subgroups B have
a remarkable structure theory for their double cosets â€“ they are indexed by the elements of a finite
Coxeter group W . For example, in the case of the group G = GLn (Fq ) and its Borel subgroup
B of upper triangular matrices, the group W is the symmetric group. There are many wonderful
references for this material; Brown [Bw], Bourbaki [Bou, Ch. IV Â§2 Ex. 22-27], Curtis and Reiner
[CR, Â§67-68] or Carter [Ca, Â§10.8-10.11]. We develop what we need in this section and give the
relation to probability theory.
     Let W be a finite Coxeter group generated by simple reflections s1 , . . . , sn . These define a
length function with â„“(id) = 0, â„“(si ) = 1, and â„“(si w) = â„“(w) Â± 1 for each w âˆˆ W , 1 â‰¤ i â‰¤ n. The
Iwahori-Hecke algebra H corresponding to W is the vector space with basis {Tw | w âˆˆ W } and
multiplication given by
                                    
                                        Tsi w ,              if â„“(si w) = â„“(w) + 1,
                        Ti Tw =                                                                 (3.14)
                                        (q âˆ’ 1)Tw + qTsi w , if â„“(si w) = â„“(w) âˆ’ 1.

where Ti = Tsi for 1 â‰¤ i â‰¤ n. When w = si , Ti2 = (q âˆ’1)Ti +q or, equivalently, (Ti âˆ’q)(Ti +1) = 0.
     Let WÌ‚ be an index set for the irreducible representations of W . For each Î» âˆˆ WÌ‚ let Ï‡Î»W
be the corresponding irreducible character of W and let dÎ» = Ï‡Î»W (1) be the dimension of this
representation. The irreducible representations of the Iwahori-Hecke algebra H are in one-to-one
correspondence with the irreducible representations of W in such a way that if Ï‡Î»H is the character
of the irreducible representation of H indexed by Î» âˆˆ WÌ‚ then
                                                     
                                            Ï‡Î»H (Tw )q=1 = Ï‡Î»W (w),

for all w âˆˆ W , see [CR, (68.21)]. In particular, the dimension of the irreducible representation of
H indexed by Î» is dÎ» .
     Define a trace ~t: H â†’ C on H by
                              
                                  PW (q), if w = 1,                             X
                  ~t(Tw ) =                                   where PW (q) =          q â„“(w)
                                  0,      otherwise,
                                                                               wâˆˆW


is the PoincareÌ polynomial of W . Then ~t is the trace on H given by (3.11) and the generic degrees
are the constants tÎ» defined by                 X
                                           ~t =      tÎ» Ï‡Î»H .                                 (3.15)
                                                     Î»âˆˆWÌ‚

where Ï‡Î»H , Î» âˆˆ WÌ‚ , are the irreducible characters of H (see (3.2)). If h, iH : H Ã— H â†’ C is the inner
product on H given by hh1 , h2 iH = ~t(h1 h2 ), for all h1 , h2 âˆˆ H, then

                         hTx , Tyâˆ’1 iH = Î´xy q â„“(y) PW (q),      for all x, y âˆˆ W ,             (3.16)

see [CR, (68.29)].
     The â€œtrivialâ€ representation Ï1 of the Iwahori-Hecke algebra H is the one-dimensional repre-
sentation corresponding to the trivial representation of W . For w âˆˆ W ,

                                                                         1    X
                     Ï1 (Tw ) = Ï‡1H (Tw ) = q â„“(w) ,        and Ï€ =             Tw
                                                                       PW (q)
                                                                             wâˆˆW
14                                    persi diaconis             and        arun ram

is the corresponding minimal central idempotent of H (cf. (3.9)). Since t1 = 1,

                                ~t(hÏ€) = t1 Ï‡1 (h),          and            Tw Ï€ = q â„“(w) Ï€,                   (3.17)

for all h âˆˆ H and w âˆˆ W , see [CR, (68.23) and (68.28)].
     Let tr be the trace of the regular representation of H, i.e. tr(h) is the trace of the linear
transformation obtained from the action of h on H by left multiplication. Then
                                                        X
                                                 tr =            dÎ» Ï‡Î»H ,                                      (3.18)
                                                        Î»âˆˆWÌ‚


where dÎ» are the dimensions of the irreducible representations of H (see [CR, (3.37)(iii)]).
Both traces tr and ~t are important in our analysis of Metropolis walks (see, for example, the proof
of Proposition 4.8).

4. Metropolis walks and systematic scans
     This section brings together previous results in the form needed to prove our main theo-
rems. We show that the various systematic scans are precisely represented as multiplication in
the Iwahori-Hecke algebra. Then representation theory yields tractable expressions for the norms
involved.

4a. Metropolis walks on W
     Let W be a finite Coxeter group generated by simple reflections s1 , . . . , sn and, for each
1 â‰¤ i â‰¤ n, let Pi (x, y) be the Markov chain on W given by
                                                        
                                                            1,    if y = si x,
                                         Pi (x, y) =
                                                            0,    otherwise.

Fix Î¸, 0 < Î¸ â‰¤ 1, and let Ï€ be as in (1.1) Then             the Metropolis construction produces the Markov
chain                               ï£±
                                    ï£² 1,      if            y = si x and â„“(y) > â„“(x),
                         Ki (x, y) = Î¸,       if            y = si x and â„“(y) < â„“(x),                           (4.1)
                                    ï£³
                                      1 âˆ’ Î¸, if             y = x.
The chain Ki can be interpreted as follows:
     From w, try to multiply by si . If this increases the length, carry out the multiplica-
     tion. If it decreases the length flip a Î¸-coin. If the coin comes out heads carry out the
     multiplication. If it comes up tails the chain stays at w.
Of course, the chain based on a fixed value of i is not irreducible. However, any convex linear
combination and any symmetric product of reversible Markov chains with a fixed stationary dis-
tribution is reversible with the same stationary distribution. If W is the symmetric group then the
following chains are reversible for Ï€:
                                 n
                              1X
                                    Ki                                              (random scan Metropolis)
                              n i=1
                    K1 K2 Â· Â· Â· Kn Kn Â· Â· Â· K2 K1                                    (short systematic scan)    (4.2)

     (K1 K2 Â· Â· Â· Kn Kn Â· Â· Â· K2 K1 ) Â· Â· Â· (K1 K2 K2 K1 )(K1K1 )                     (long systematic scan)
                             Metropolis scans and Hecke algebras                                                      15

Note that K1 K2 Â· Â· Â· Kn is an irreducible Markov chain with Ï€ stationary. However, it is not
reversible in general.
     The following Theorem (which follows directly from our setup) shows that many Markov chains
on W can be obtained by left multiplication by elements of H on the basis {TÌƒw }. The remaining
results in this subsection provide the necessary tools for studying the convergence of these chains
by using the representation theory of the Iwahori-Hecke algebra H. Though we have chosen to
focus here on the Iwahori-Hecke algebras related to finite reflection groups W , the results of this
section hold in a general Hecke algebra context.

Theorem 4.3. Let W be a finite Coxeter group and let H be the Iwahori-Hecke algebra with
basis {Tw }wâˆˆW as defined in (3.14). Let

                q = Î¸ âˆ’1 ,         TÌƒi = Ti /q,          and       TÌƒw = q âˆ’â„“(w) Tw ,      for w âˆˆ W .

The Metropolis chain Ki in (4.1) is the same as the matrix of left multiplication by TÌƒi with respect
to the basis {TÌƒw }wâˆˆW of H:
                                         
                                             TÌƒsi w ,                   if â„“(si w) > â„“(w),
                             TÌƒi TÌƒw =                                                                             (4.4)
                                             (1 âˆ’ Î¸)TÌƒw + Î¸ TÌƒsi w ,    if â„“(si w) < â„“(w).


    Identify functions f : X â†’ R in L2 (Ï€) with elements of the Iwahori-Hecke algebra H via
                                                          X
                                                    f=          f (x)TÌƒx .                                         (4.5)
                                                          xâˆˆW


The following Proposition shows that we can use the inner product h, iH on H (defined in Section
3c) to compute norms in L2 (Ï€). Coupled with Lemma 2.3 it gives bounds on rates of convergence
in total variation distance.

Proposition 4.6. Let W be a finite Coxeter group and let Ï€ be as in (1.1). With the identification
of L2 (Ï€) and the Iwahori-Hecke algebra H given by (4.5),

                             hf /Ï€, g/Ï€i2 = hf, g âˆ— iH ,            for all f, g âˆˆ L2 (Ï€).

where âˆ— : H â†’ H is the involutive anti-automorphism of H defined by Twâˆ— = Twâˆ’1 .
                                     X                    X                          X
Proof. Use the notation f =                  f (x)TÌƒx =         f (x)q âˆ’â„“(x)Tx =           fx Tx . Then, since Î¸ = q âˆ’1 ,
                                    xâˆˆW                   xâˆˆW                        xâˆˆW
(2.2) and (1.1) give

                         X f (x)g(x)   X fx q â„“(x) gx q â„“(x)              X
        hf /Ï€, g/Ï€i2 =               =          âˆ’â„“(x)
                                                             PW (Î¸ âˆ’1 ) =   fx gx q â„“(x) PW (q).
                              Ï€(x)            Î¸
                         xâˆˆW                      xâˆˆW                                    xâˆˆW


Thus, by (3.16),
                             X                                  X
          hf /Ï€, g/Ï€i2 =           fx gx hTx , Tyâˆ’1 iH =               fx gy hTx , Tyâˆ’1 iH = hf, g âˆ— iH .
                             xâˆˆW                               x,yâˆˆW
16                                    persi diaconis        and     arun ram

     The following lemma shows that the inner product in L2 (Ï€), reversibility, and the involution
âˆ—: H â†’ H are simply related.

Lemma 4.7.          Let H be the Iwahori-Hecke algebra corresponding to a finite real reflection
group W and let Ï€ be as in (1.1). Let K be a reversible Markov chain on W determined by
left multiplication
           X        by an element of H (also denoted by K). The chain K operates on L2 (Ï€) by
Kf (x) =       K(x, y)f (y). Then the following are equivalent:
            yâˆˆW
(a) K is reversible with respect to Ï€,
(b) K is self adjoint with respect to h, i2 ,
 (c) K = K âˆ— in the Iwahori-Hecke algebra H,
where h, i2 is the norm on L2 (Ï€) defined in (2.2) and âˆ— : H â†’ H is the involutive anti-automorphism
of H defined by Twâˆ— = Twâˆ’1 .

Proof. If K is reversible then
                       X                           X
          hKf, gi2 =        K(x, y)f (y)g(x)Ï€(x) =   f (y)K(y, x)g(x)Ï€(y) = hf, Kgi2 ,
                       x,yâˆˆX                                x,yâˆˆX


and, conversely, if K is self adjoint then

                         Ï€(x)K(x, y) = hÎ´x , KÎ´y i = hKÎ´x, Î´y i = Ï€(y)K(y, x),

where Î´z denotes the delta function at z, given by Î´z (x) = Î´zx (Kronecker delta). So K is reversible
if and only if K is self adjoint.
     If K is self adjoint with respect to h, i2 then, by Proposition 4.6,

               hKf, g âˆ— iH = hKf /Ï€, g/Ï€i2 = hf /Ï€, Kg/Ï€i2 = hf, (Kg)âˆ—iH = hf, g âˆ— K âˆ— iH .

Thus, for all w âˆˆ W , hK, Tw iH = h1, Tw K âˆ— iH = hTw , K âˆ— iH = hK âˆ—, Tw iH . So K = K âˆ— .

    The following Proposition is a primary tool for studying rates of convergence of Markov chains
on Iwahori-Hecke algebras. It bounds the L2 (Ï€) norm of Lemma 2.3 in terms of characters of the
Iwahori-Hecke algebra. In contrast with the way that random walks are often analyzed (see, for
example [DS2]) the following Proposition also shows that the Markov chain given by K can be
analyzed without knowing the eigenvalues of K â€“ it is only necessary to compute traces.

Proposition 4.8. Let H be the Iwahori-Hecke algebra corresponding to a finite real reflection
group W . Let K be a reversible Markov chain on W with stationary distribution Ï€ determined
by left multiplication by an element of H (also denoted by K). Let Kxâ„“ denote the Markov chain
started at x after â„“ steps. Then
                               X
 (a) kKxâ„“ /Ï€ âˆ’ 1k22 = q âˆ’2â„“(x)   tÎ» Ï‡Î»H (Txâˆ’1 K 2â„“ Tx ),
                               Î»6=1
      X                            X
(b)         Ï€(x)kKxâ„“ /Ï€ âˆ’ 1k22 =          dÎ» Ï‡Î»H (K 2â„“ ),
      xâˆˆW                          Î»6=1
        Î»
where Ï‡H are the irreducible characters, tÎ» the generic degrees (3.15), and dÎ» the dimensions of
the irreducible representations of H.
                           Metropolis scans and Hecke algebras                                         17

Proof. Equation (3.17) says ~t(hÏ€) = t1 Ï‡1H (h), and TÌƒw Ï€ = Ï€, for all h âˆˆ H and w âˆˆ W . Thus, since
Kxâ„“ is a probability, Proposition 4.6 gives

                   1 = hKx2â„“ /Ï€, 1i2 = hK 2â„“ TÌƒx , Ï€iH = hK 2â„“ TÌƒx , TÌƒxâˆ’1 Ï€iH
                     = ~t(K 2â„“ TÌƒx TÌƒxâˆ’1 Ï€) = t1 Ï‡1H (K 2â„“ TÌƒx TÌƒxâˆ’1 ) = t1 Ï‡1H (TÌƒxâˆ’1 K 2â„“ TÌƒx ).

Then, by Proposition 4.6,
                  
 â„“            D                      E  D                        E
                   Kx /Ï€, Kxâ„“ /Ï€ = K â„“ TÌƒx , (K â„“ TÌƒx )âˆ—   = K â„“ TÌƒx , TÌƒxâˆ’1 (K â„“ )âˆ—   .
                                    2
                                                                H                                  H


Thus, by (3.12) and Lemma 4.7(c),
    
 â„“              D                  E                              X
     Kx /Ï€, Kxâ„“ /Ï€ = K â„“ TÌƒx , TÌƒxâˆ’1 K â„“   = ~t TÌƒxâˆ’1 K 2â„“ TÌƒx = q âˆ’2â„“(x)   tÎ» Ï‡Î»H (Txâˆ’1 K 2â„“ Tx ).
                    2
                                                H
                                                                                      Î»âˆˆWÌ‚
                      
                         
             
Now (a) follows since Kxâ„“ /Ï€ âˆ’ 1, Kxâ„“ /Ï€ âˆ’ 1 2 = Kxâ„“ /Ï€, Kxâ„“ /Ï€ 2 âˆ’1. Part (b) follows similarly from
the following calculation. Using the definition (2.2) of the norm on L2 (Ï€),
      X        
                X      K â„“(x, y)K â„“ (x, y)   X      K â„“ (y, x)K â„“ (x, y)
           Ï€(x) Kxâ„“ /Ï€, Kxâ„“ /Ï€ =   Ï€(x)                     =   Ï€(y)
                               2               Ï€(y)                          Ï€(y)
     xâˆˆW                                x,yâˆˆW                                 x,yâˆˆW


                                        X                                 X
                                   =          K 2â„“ (y, y) = tr(K 2â„“ ) =          dÎ» Ï‡Î»H (K 2â„“ ),
                                        yâˆˆW                               Î»âˆˆWÌ‚

where tr is the trace of the regular representation of H given in (3.18).


4b. Systematic scans
     One case of Proposition 4.8 which can be analyzed for all finite Coxeter groups W is the case
when the Markov chain K is a (generalized) systematic scan. This is when K is given by left
multiplication by the element TÌƒw2 0 in the Iwahori-Hecke algebra. In terms of the geometry of the
Coxeter group this chain is the Metropolis walk on the chambers which tries to move a chamber
to its opposite chamber and back again by successive reflections in the walls of chambers. Since
each step is a Metropolis step the chance that the chamber returns to its original position after
one pass is not 1, but depends on the parameter Î¸. In the case when W is the symmetric group
this chain is the long systematic scan defined in (4.2).
     Let z be the sum of all the reflections in W . Then z is a central element (since it is a conjugacy
class sum) in the group algebra of W and thus, by Schurâ€™s Lemma, z acts by a constant cÎ» on the
irreducible representation of W labeled by Î» âˆˆ WÌ‚ . The following well known result shows that the
element TÌƒw2 0 , where w0 is the longest element of W , is an Iwahori-Hecke algebra analogue of the
element z. From the point of view provided by Theorem 4.3 the following Proposition determines
the eigenvalues (with their multiplicities) of the systematic scan Metropolis chain K on W .

Proposition 4.9. Let z be the sum of all the reflections in W and let w0 be the longest element
of W . The element TÌƒw2 0 is in the center of the Iwahori-Hecke algebra H and

                                                                                 Ï‡Î»W (z)
                           ÏÎ» (TÌƒw2 0 ) = q cÎ» âˆ’â„“(w0 ) Id,    where cÎ» =                 ,
                                                                                   dÎ»
18                                   persi diaconis               and   arun ram

ÏÎ» is the irreducible representation of H indexed by Î», Ï‡Î»W is the irreducible character of W labeled
by Î», and dÎ» = Ï‡Î»W (1) is the dimension of this representation.

Proof. This result is standard, see [Ra, (2.4) and (2.5)], so we only sketch the proof here. A
result of Brieskorn-Saito [BS] and Deligne [De] says that Tw2 0 is in the center of the corresponding
braid group. Since the Iwahori-Hecke algebra H is a quotient of the group algebra of the braid
group it follows that Tw2 0 is in the center of H. The constant by which it acts on the irreducible
representation labeled by Î» can be checked as follows. The element Tw2 0 âˆ’ q â„“(w0 ) is divisible by
(q âˆ’ 1) and
                       T 2 âˆ’ q â„“(w0 )              q â„“(w0 )+cÎ» âˆ’ q â„“(w0 ) 
                   z = w0               .     Since                              = cÎ» ,
                            qâˆ’1         q=1                  qâˆ’1               q=1

and z acts by the constant cÎ» the element Tw2 0 must act by the constant q â„“(w0 )+cÎ» . The result of
the Proposition now follows since TÌƒw2 0 = q âˆ’2â„“(w0 ) Tw2 0 . An alternative way to obtain the constant
q â„“(w0 )+cÎ» which Tw2 0 acts by is to note that

                                                                  Î»          Î»
                              det(ÏÎ» (Ti )) = (âˆ’1)(dÎ»âˆ’Ï‡W (si ))/2 q (dÎ» +Ï‡W (si ))/2 ,

for all 1 â‰¤ i â‰¤ n, and this and [Bou, Ch. VI Â§1 Cor. 2] imply that
                                                           Î»
                         det(Tw2 0 ) = q 2â„“(w0 )dÎ» +2Ï‡W (z))/2 = q dÎ» (â„“(w0 )+cÎ» ) .

     Combining Propositions 4.9 and 4.8 gives the following bounds on the convergence of the
systematic scan Metropolis walk on a finite Coxeter group W . Explicit analyses of these bounds
in examples are given in Sections 5,6,7.

Theorem 4.10. Let H be the Iwahori-Hecke algebra corresponding to a finite real reflection group
W . Let K be the systematic scan Metropolis chain on W , i.e. the reversible Markov chain on W
with stationary distribution Ï€ determined by left multiplication by the element TÌƒw2 0 of H where
w0 is the longest element of W . Then
                     X
(a) kK1â„“ /Ï€ âˆ’ 1k22 =    tÎ» dÎ» Î¸ 2â„“(â„“(w0 )âˆ’cÎ» ) ,
                       Î»6=1
      X                            X
(b)         Ï€(x)kKxâ„“ /Ï€ âˆ’ 1k22 =          d2Î» Î¸ 2â„“(â„“(w0)âˆ’cÎ» ) ,
      xâˆˆW                          Î»6=1

where â„“(w0 ) is the length of w0 , tÎ» are the generic degrees (see (3.15)), dÎ» the dimensions of the
irreducible representations of H, and the constants cÎ» are as given in Proposition 4.9.


5. The hypercube
     We begin with a simple but instructive example where all details can be carried out. We are
able to analyze and compare both randomized and systematic scans. We show that both kinds of
scans take order n log n operations to converge to stationarity. For small values of Î¸ the systematic
scan converges faster and for Î¸ close to 1 the random scan converges faster.

5a. Preliminaries
                            Metropolis scans and Hecke algebras                                      19

    The Coxeter group W = (Z/2Z)n has generators s1 , s2 , . . . , sn and relations

                             s2i = 1,       si sj = sj si ,       for all 1 â‰¤ i, j â‰¤ n.

The set X = W = (Z/2Z)n is the space of binary n-tuples, si is the vector with 1 in the ith
coordinate and 0 elsewhere, and the length function is given by â„“(x) = |x| = (# of ones in x). The
longest element of W is w0 = s1 s2 Â· Â· Â· sn and â„“(w0 ) = n.
    The irreducible representations ÏÎ» of the Iwahori-Hecke algebra of (Z/2Z)n are all one-
dimensional and are indexed by n-tuples Î» = (Î»1 , . . . , Î»n ), Î»i âˆˆ {0, 1}. Let |Î»| = Î»1 + Â· Â· Â· + Î»n .
Then                    
               Î»          q,   if Î»i = 0,
              Ï (Ti ) =                           cÎ» = n âˆ’ 2|Î»|,     and      tÎ» = q |Î»| ,        (5.1)
                          âˆ’1, if Î»i = 1,
where cÎ» and tÎ» are the constants defined in Proposition 4.9 and (3.15), respectively.
    Fix 0 < Î¸ â‰¤ 1 and let

                             q â„“(x)
                   Ï€(x) =           ,       where q = Î¸ âˆ’1               and PW (q) = (1 + q)n    (5.2)
                            PW (q)

is a normalizing constant. Then Ï€ is a product measure on (Z/2Z)n since
                                                          â„“(x)           nâˆ’â„“(x)
                                                 q                     1
                                  Ï€(x) =                                              .
                                                1+q                   1+q


5b. Random scan Metropolis
    The random scan Metropolis algorithm proceeds by choosing a coordinate at random and
attempting to change to its opposite mod 2. If this results in a one, the change is made. If the
change in a zero, flip a coin with parameter Î¸. If the flip comes up heads change the chosen
coordinate to 0 and if it comes up tails then the coordinate stays at 1. The resulting chain is
                                   ï£±
                                   ï£´ (1/n),             if â„“(y) = â„“(x) + 1,
                                   ï£´
                                   ï£´
                                   ï£´
                                   ï£´
                                   ï£´
                                   ï£² (1/n)Î¸,
                                   ï£´                    if â„“(y) = â„“(x) âˆ’ 1,
                         K(x, y) =                                                             (5.3)
                                   ï£´
                                   ï£´
                                   ï£´
                                   ï£´ (â„“(x)/n)(1  âˆ’ Î¸),  if y = x,
                                   ï£´
                                   ï£´
                                   ï£´
                                   ï£³
                                     0,                 otherwise.
The following theorem shows that order n log n steps are necessary and sufficient to reach station-
arity.

Theorem 5.4. Let the random scan Metropolis algorithm on (Z/2Z)n be defined by (5.3) with
0 < Î¸ â‰¤ 1. Then, for any starting state x, and any â„“,
                                                                                     2â„“
                                                X                         |Î»|
                          kKxâ„“ /Ï€       2
                                    âˆ’ 1k2 =            Î¸   2Î»Â·xâˆ’|Î»|
                                                                       1âˆ’     (1 + Î¸)     .       (5.5)
                                                                           n
                                                Î»6=0

For 0 < Î¸ < 1 and â„“ = n(log n âˆ’ log Î¸ + c)/2(1 + Î¸) with c > 0,
                                                âˆ’c     
                               kKxâ„“ âˆ’ Ï€k2T V â‰¤ ee âˆ’ 1 + eâˆ’c/2 .                                   (5.6)
20                                                 persi diaconis                 and      arun ram

The bound in (5.6) is sharp in the sense that if â„“ = n(log n âˆ’ log Î¸ + c)/2(1 + Î¸) then for all Ç« > 0
there exists a c < 0 such that kK0â„“ âˆ’ Ï€kT V > 1 âˆ’ Ç« for all sufficiently large n.

Proof. From the definitions of the irreducible representations of H,
                                                                    
          Î»
         Ï‡H TÌƒ         xâˆ’1   (( n1 )(TÌƒ1   + Â· Â· Â· + TÌƒn )) TÌƒx = nâˆ’2â„“ q âˆ’2â„“(x)âˆ’2â„“ Ï‡Î»H (T1 + Â· Â· Â· + Tn )2â„“ Ï‡Î»H (Tx )2
                                                           2â„“

                                                                                                           2â„“
                                                                         = nâˆ’2â„“ q âˆ’2â„“ ((n âˆ’ |Î»|)q âˆ’ |Î»|) q 2(|x|âˆ’Î»Â·x) q âˆ’2|x|
                                                                                                    2â„“
                                                                         = (1 âˆ’ (|Î»|/n)(1 + Î¸)) Î¸ 2Î»Â·x .

The first statement then follows from Theorem 4.10(a) with the value for tÎ» given in (5.1). For the
second statement we need to bound the sum on the right hand side of (5.5). Since Î¸ â‰¤ 1, Î¸ Î»Â·x â‰¤ Î¸ 0
and
                                          n                     2â„“
                              â„“      2
                                         X   n âˆ’j         j
                          kKx /Ï€ âˆ’ 1k2 â‰¤         Î¸    1 âˆ’ (1 + Î¸)      .
                                         j=1
                                             j            n
                                                                           n
                                                                            
Break the sum at n/2. For the first half use                               j     â‰¤ nj /j! and 1 âˆ’ x â‰¤ eâˆ’x to give an upper bound

                                           n/2                       n/2
                                           X   1  n j âˆ’j(1+Î¸)2â„“/n X eâˆ’jc      âˆ’c
                                                       e           =        â‰¤ ee âˆ’ 1.
                                               j! Î¸                      j!
                                           j=1                                       j=1


For the second half change j to n âˆ’ k and use the same inequalities to get an upper bound

              n/2 k                                              n/2
              X  n               kâˆ’n âˆ’(nâˆ’k)(1+Î¸)2â„“/n
                                                                 X   1 k(log n+log Î¸+2â„“(1+Î¸)/n)âˆ’n log Î¸âˆ’2â„“n(1+Î¸)/n
                             Î¸      e                          =        e
                       k!                                            k!
              k=0                                                   k=0

        Pm    Ak
Using     k=0 k!         â‰¤ Am for A â‰¥ 2 the bound for the second half becomes

         e(n/2)(log n+log Î¸+2â„“(1+Î¸)/n)âˆ’n log Î¸âˆ’2â„“n(1+Î¸)/n = e(n/2)(log nâˆ’log Î¸âˆ’2â„“(1+Î¸)/n) = eâˆ’nc/2 .

     To show that this upper bound is sharp we use the second moment method. With respect to
the action of K on L2 (Ï€) defined in Lemma 4.7 the matrix K(x, y) of (5.3) has an orthonormal
basis of eigenfunctions

                                                                                           |Î»|
        fÎ» (y) = Î¸ âˆ’|Î»|/2 (âˆ’Î¸)Î»Â·y ,                      with eigenvalues 1 âˆ’                  (1 + Î¸),       Î» âˆˆ (Z/2Z)n .     (5.7)
                                                                                            n

Let ei âˆˆ (Z/2Z)n be the vector with 1 in the ith entry and 0 elsewhere. We shall use the test
function,
                                                  n                                                                     
               X
                                           âˆ’1/2
                                                  X
                                                               yi         âˆ’1/2                      n         |y|(1 + Î¸)
     T (y) =            fei (y) = Î¸                     (âˆ’Î¸)        =Î¸           (n âˆ’ |y|(1 + Î¸)) = âˆš      1âˆ’              .    (5.8)
                   i                              i=1
                                                                                                     Î¸             n

The expectation and the variance of T with respect to the distribution Ï€ are

                                           EÏ€ (T ) = 0              and          VarÏ€ (T ) = EÏ€ (T 2 ) = n.                     (5.9)
                              Metropolis scans and Hecke algebras                                21

For i 6= j,
                                                                          1âˆ’Î¸
                             fei fej = fei +ej ,      and      fe2i = f0 + âˆš fei ,
                                                                            Î¸
where the second identity is verified by checking that both sides agree when evaluated at each of
the two cases: y such that yi = 1 and y such that yi = 0. We can compute the expectation and
variance of T under the distribution K0â„“ as follows:

                                             n                              â„“
                    X                        X                  n        1+Î¸
      Eâ„“,0 (T ) =       K â„“ (0, y)T (y) =        (K â„“fei )(0) = âˆš     1âˆ’        ,    and      (5.10)
                    y                                            Î¸        n
                                             i=1


    Varâ„“,0 (T ) = Eâ„“,0 (T 2 ) âˆ’ Eâ„“,0 (T )2
                       ï£«                            ï£¶
                           n                              2
                                                                      2â„“
                         X
                                2
                                      X                 n        1 + Î¸
                = Eâ„“,0 ï£­       fe i +     fe i fe j ï£¸ âˆ’       1âˆ’
                          i=1
                                                        Î¸          n
                                      i6=j
                                   â„“                        â„“              2â„“
                   n(1 âˆ’ Î¸)     1+Î¸      n(n âˆ’ 1)     2(1 + Î¸)      n2      1+Î¸
               =n+           1âˆ’        +           1âˆ’             âˆ’      1âˆ’         .
                      Î¸          n          Î¸            n          Î¸        n
    We want to use these formulas to show that â„“ = (n/2(1 + Î¸))(log n âˆ’ log Î¸ + c) steps are sharp.
                                                           2
Fixing k and using log(1 âˆ’ x) = âˆ’x âˆ’ x2 /2 + O(x3 ) and eâˆ’x /2 = 1 âˆ’ x2 /2 + O(x4),
                     â„“                                     âˆ’c k/2                  
             k(1 + Î¸)       â„“(âˆ’k(1+Î¸)/nâˆ’â„“k 2(1+Î¸)2 /2n2 )    Î¸e           â„“k 2 (1 + Î¸)2
          1âˆ’             âˆ¼e                               âˆ¼            1âˆ’                 ,
                n                                             n                 2n2

when n is large. Thus, for â„“ = (n/2(1 + Î¸))(log n âˆ’ log Î¸ + c) and n large,
                âˆš âˆ’c/2
    Eâ„“,0 (T ) âˆ¼   ne     , and
                    r                                                                        
                      n           âˆ’c/2            âˆ’c      â„“4(1 + Î¸)2     n2 Î¸ âˆ’c     2â„“(1 + Î¸)2
  Varâ„“,0 (T ) âˆ¼ n +      (1 âˆ’ Î¸)e      + (n âˆ’ 1)e     1âˆ’               âˆ’     e    1âˆ’
                      Î¸                                       2n2        Î¸ n            2n2
                                                     
                          âˆš                â„“2(1 + Î¸)2
              âˆ¼ n + Oc,Î¸ ( n) + neâˆ’c âˆ’                  âˆ’ eâˆ’c
                                              2n2
                          âˆš
              = n + Oc,Î¸ ( n) + Oc,Î¸ (log n),

with the error terms depending on c and Î¸. By first choosing c to be a fixed (large) âˆš  negative
number and then letting n â†’ âˆž, we see that, if b is large, the set Ab = {x | |T (x)| â‰¤ b n} has
probability 1 âˆ’ 1/b2 under Ï€ and probability O(1/b2) under K0â„“ . This completes the proof of the
last statement.


5c. Systematic scan Metropolis

     We turn next to the systematic scan version. Order n log n steps are required here too. Lest
the reader think this contradicts the example which begins this paper, we note that the opening
example (which actually corresponds to the heat bath updating setup) replaces each coordinate
with a freshly chosen pick. Thus a zero coordinate which is chosen can remain zero with probability
Î¸. For the Metropolis version analyzed here a chosen zero must change to a one.
22                                   persi diaconis        and        arun ram
                                                                                                     
                                                                                              0    1
     With notation as in Section 5a, let N be the chain on Z/2Z with matrix                             . On
                                                                                              Î¸   1âˆ’Î¸
(Z/2Z)n define Ki acting as N on the ith coordinate. Let

                                         K = K1 K2 Â· Â· Â· Kn Kn Â· Â· Â· K1 .                              (5.11)

This is the systematic scan Metropolis algorithm with stationary distribution Ï€. The following
theorem gives bounds on the distance to stationarity.

Theorem 5.12. Let the systematic scan Metropolis algorithm on (Z/2Z)n be defined by (5.11)
with 0 < Î¸ â‰¤ 1. Then, for any starting state x and any â„“
                                                         X
                                     kKxâ„“ /Ï€ âˆ’ 1k22 =          Î¸ (4â„“âˆ’1)|Î»|+2Î»Â·x .                      (5.13)
                                                        Î»6=0

                                               
                         1       log n + c
For, 0 < Î¸ < 1 and â„“ =                     +1       with c > 0,
                         4       log(1/Î¸)

                                                   1  eâˆ’c       
                                       kKxâ„“ âˆ’ Ï€k2T V â‰¤ e     âˆ’1 .                             (5.14)
                                                   4
                                                                           
                                                          1 log n + c
The bound in (5.14) is sharp in the sense that if â„“ =                   + 1 then for all Ç« > 0 there
                                                          4 log(1/Î¸)
exists a c < 0 such that kK0â„“ âˆ’ Ï€kT V > 1 âˆ’ Ç« for all sufficiently large n.

Proof. From the definitions of the ireducible representations of H,
                                                         
                                      Ï‡Î»H TÌƒxâˆ’1 TÌƒw2â„“0 TÌƒx = Î¸ 4â„“|Î»|+2Î»Â·x .

The first statement then follows from Proposition 4.8(a). Since Î¸ â‰¤ 1, Î¸ 2Î»x â‰¤ Î¸ 0 , and

                                                n                              n
                    kKxâ„“ /Ï€ âˆ’ 1k22 = 1 + Î¸ 4â„“âˆ’1 âˆ’ 1 = 1 + e(4â„“âˆ’1) log Î¸ âˆ’ 1
                                     (4â„“âˆ’1) log Î¸ n          log n+(4â„“âˆ’1) log Î¸
                                   â‰¤ ee               âˆ’ 1 = ee                    âˆ’ 1,

and the upper bound follows.
    The proof of the lower bound is similar to the random scan case and we only give the variants
needed. As in (5.7) the eigenvectors of the Markov chain are

           fÎ» (y) = Î¸ âˆ’|Î»|/2 (âˆ’Î¸)Î»Â·y ,        but with eigenvalues Î¸ 2â„“|Î»| ,        Î» âˆˆ (Z/2Z)n .

Use the same test statistic T as in (5.8). The expectation and the variance of T with respect to
the distribution Ï€ are the same as in (5.9) and a calculation similar to that in (5.10) yields that
the expectation and variance of T under the distribution M0â„“ are
                                                                                       
                          n                                               1 âˆ’ Î¸ 2â„“ 1 4â„“
              Eâ„“,0 (T ) = âˆš Î¸ 2â„“ ,         and        Varâ„“,0 (T ) = n 1 +      Î¸ âˆ’ Î¸      .
                           Î¸                                                Î¸      Î¸
                            Metropolis scans and Hecke algebras                               23

               1
For â„“ =              (log n + c),
          4 log(1/Î¸)

              âˆš                                                      
               n                                   1 âˆ’ Î¸ âˆ’c/2    1 âˆ’c                   âˆš
  Eâ„“,0 (T ) = âˆš eâˆ’c/2 ,     and Varâ„“,0 (T ) = n 1 + âˆš e       âˆ’    e    = n(1 + Oc,Î¸ (1/ n)).
                Î¸                                  Î¸ n          Î¸n


This allows the argument to conclude as in the proof of Theorem 5.4.


    After â„“ passes the systematic scan algorithm makes 2â„“n basic steps. Thus the results of
Theorems 5.4 and 5.12 show that both scanning strategies converge in order n log n basic steps.
The following table compares the lead term constants for the two scanning strategies at various
values of Î¸.
                               Î¸                  random                 systematic

                                                 n log(n/Î¸)                 n log n
                            general
                                                  2(1 + Î¸)                2 log(1/Î¸)
                              1                    n log n                  n log n
                             1+Ç«                      4                  2 log(1 + Ç«)
                               1                  n log 2n                  n log n
                               2                      3                     2 log 2
                                                 n log(n/Ç«)                 n log n
                               Ç«
                                                      2                   2 log(1/Ç«)

We see that the lead term constants make the random scan faster as Î¸ â†’ 1 while the systematic
scan is faster as Î¸ â†’ 0.


6. The dihedral group.

     The hypercube of Section 5 is commutative. This section treats the simplest noncommutative
example â€“ the dihedral group D2n . We completely analyze the convergence of both the randomized
and systematic scans. We find that both scanning strategies take order n operations to converge
to stationarity.


6a. Preliminaries

    The dihedral group of order 2n is the group W given by generators s1 , s2 and relations


                           s21 = 1,   s22 = 1,     and   s1 s2 s1 Â· Â· Â· = s2 s1 s2 Â· Â· Â· .
                                                         | {z } | {z }
                                                           n factors        n factors



This is the group of symmetries of a regular 2n-gon where s1 and s2 act by reflection in axes
24                                                            persi diaconis                                                                                           and                                    arun ram

through the center of the 2n-gon which form an angle of 2Ï€/2n.


                                                                                                                                                                                                                         ...
                                                                                                                                                                                                                     ...
                                                                                                                                                                                                                  ..
                                                                                                                                                                                                              ....
                                                                                                                                                                                                           ...
                                                                                                                                                                                                        ...
                                                                                                                                                                                                       ..
                                                                                      ..................................................................................................................
                                                                                      .
                                                                                   ... .                                                                                                       ... ...
                                                                               ... .                                                                                                         ... .....
                                                                            ...                   .                                                                                      ..
                                                                                                                                                                                           ..                ...
                                                                        ...                          .                                                                                 ..                       ...
                                                                      ..                                                                                                              ..                           ...
                                                              ..
                                                                 ....                                    .
                                                                                                            .
                                                                                                                    s         1                          id                     ..
                                                                                                                                                                                  ....                                ...
                                                                                                                                                                                                                          ...
                                                             .                                                                                                                  .                                            ...
                                                           ..
                                                           .                                                   .                                                             ..
                                                                                                                                                                              .                                                 ...
                                                      ..
                                                       .
                                                         ..
                                                         .                      s s         1           2         .
                                                                                                                     .                                                  .
                                                                                                                                                                        ..
                                                                                                                                                                           ..
                                                                                                                                                                           .          s       2                                   ...
                                                                                                                                                                                                                               . .....
                                                   ... . .                                                                                                           ...                                               .               ...
                                                ...                       .                                             .
                                                                                                                                                                  ....                                          .                        ...
                                              ...                                . .                                       .                                     ..                                . .                                      ...
                                            ..                                                 .                               .                               ..                             .                                                ...
                                          ...
                                          .                                                         .                             .                        .
                                                                                                                                                           ....                         .                                                        ...
                                        .
                                        .                                                                  .                                             .
                                                                                                                                                         .                         .                                                               ...
                                                                                                               .                                                              .
                                 .......
                                                         s s s1 2 1                                                  . . .
                                                                                                                                .       . .. . .
                                                                                                                                                   .
                                                                                                                                                     ....               .                        s s     2 1
                                                                                                                                                                                                                                                     ...
                                                                                                                                                                                                                                                        ...
                                                                                                                                                                                                                                                          ...
                               ...                                                                                                   . . ... .                                                                                                              .
                            .... . . . . . . . . . . . . . . . .. ...... .. . . . . . . . . . . . . . . . ......
                               ...                                                                                                      .   . ..
                                                                                                                                            .                                                                                                               .
                                 ...                                                                                              . ..     .              .                                                                                               ..
                                                                                                                                                                                                                                                          .
                                   ...
                                      ...
                                        ...
                                                   s s s s
                                                         1 2 1 2 . . . ......
                                                                                                                             .          .           .
                                                                                                                                                       .
                                                                                                                                                                .
                                                                                                                                                                     .
                                                                                                                                                                          .
                                                                                                                                                                                .
                                                                                                                                                                                           s s s  2            1               2                   ....
                                                                                                                                                                                                                                                       ...

                                                                                                                                ...                        .                                                                                     ...
                                          ...
                                            ...                                                   . .                         ...                             .
                                                                                                                                                                                     . .
                                                                                                                                                                                                                                              ...
                                                                                            .                                                                                                   .
                                              ...
                                                ...                           .
                                                                                     .
                                                                                                                          ....                                   .                                     .
                                                                                                                                                                                                            .                            . ...
                                                  ... . .                                                                .
                                                                                                                         .                                          .                                              .                    ..
                                                     ... .                                                            ...                                              .                                                   . . ....
                                                       ...                                                         ...                                                    .                                                         ..
                                                         ...                                                     ..                                                                                                               ..
                                                           ...                                                ....                                                           .                                                 ...
                                                             ...                                            .
                                                                                                            ..                                                                  .                                          ....
                                                                 ...                                     ...                                                                       .                                   ...
                                                                    ...                                ..                                                                                                           ..
                                                                       ...
                                                                          ...                       ...                                                                                .                         ...
                                                                             ...                 .
                                                                                                 ...                                                                                      .                  .
                                                                                                                                                                                                             ...
                                                                                ... ...                                                                                                      .            ..
                                                                                   ... ...                                                                                                      . ..
                                                                                     ....................................................................................................................
                                                                                     .
                                                                                     ..
                                                                                 ...
                                                                              ..
                                                                           ...
                                                                       .
                                                                       ...
                                                                   ...
                                                               ...




     Fix 0 < Î¸ â‰¤ 1 and consider the distribution on W given by

                      q â„“(w)                                                                                                                                         (q 2 âˆ’ 1) (q n âˆ’ 1)
            Ï€(w) =           ,                                  where PW (q) =                                                                                                                                                                                  and q = Î¸ âˆ’1 .
                     PW (q)                                                                                                                                           (q âˆ’ 1) (q âˆ’ 1)

This measure is largest at the longest element of W , w0 = s1 s2 s1 Â· Â· Â· (n factors), and â„“(w0 ) = n.
The walks to be analyzed will all start at the identity.
      One may picture the walks described in this section on the 2n chambers of an n-gon. Pick
one chamber (labeled with identity) and identify the two internal sides with s1 , s2 . Reflecting the
fundamental chamber around gives each edge and each chamber a label. The distance â„“(w) is the
smallest number of chambers required to walk from the chamber labeled by w to the identity. For
example, in D12 , pictured above â„“(s1 s2 s1 s2 ) = 4.
      The random scan Metropolis walk proceeds from w by choosing one of s1 , s2 with probability
1/2. If â„“(si w) > â„“(w) the move is accepted. If â„“(si w) < â„“(w) the move is accepted with probability
Î¸ and rejected with probability 1 âˆ’ Î¸.
      One pass of the systematic scan Metropolis algorithm chooses n generators in the order
s1 , s2 , s1 , s2 , . . .. Geometrically, starting from the identity, this amounts to marching around the
n-gon. If no rejections are made one complete scan ends in w0 .
      Our bounds result in explicit expressions for the convergence of the two walks. One of these
has been carefully analyzed by Belsey [Be, Ch. VI Thm. 2-10]. He showed

Proposition 6.1. For the random scan Metropolis algorithm starting from the identity
                                          r
                         â„“       2     âˆ’n   1+Î¸            âˆš 2â„“
                      kK1 /Ï€ âˆ’ 1k2 â‰¤ Î¸           1 âˆ’ 12 (1 âˆ’ Î¸)2    .                                                                                                                                                                                                                      (6.2)
                                            1âˆ’Î¸
                                                                                                                                                                                                                                                                   n log Î¸
For 0 < Î¸ < 1 the right hand side of (6.2) is small for k of order                                                                                                                                                                                                              âˆš      .
                                                                                                                                                                                                                                                          2 log(1 âˆ’ (1/2)(1 âˆ’    Î¸)2 )
                             Metropolis scans and Hecke algebras                                                   25

Belsley [Be, Ch. VI Thm. 4-12] further shows that the random scan Metropolis algorithm has a
total variation cutoff at
                                           2n      âˆš
                                      â„“=       + c n.
                                          1âˆ’Î¸
       For the systematic scan algorithm our results show
                                                        2              
                     â„“        2     (4â„“âˆ’1)n    (2â„“âˆ’1)n  Î¸ âˆ’ 1 Î¸n âˆ’ 1
                   kK1 /Ï€ âˆ’ 1k2 = Î¸         +Î¸               Â·       âˆ’ 1 âˆ’ Î¸ 2â„“n ,
                                                         Î¸âˆ’1 Î¸âˆ’1

and so, when â„“ = 1,
                                                                             
                                                          Î¸ 2 âˆ’ 1 âˆ’1                  2Î¸ n+1
                            kK1â„“ /Ï€       2
                                      âˆ’ 1k2 â‰¤ Î¸   n
                                                                 Â·   âˆ’1           =          .                   (6.3)
                                                          1âˆ’Î¸ Î¸âˆ’1                     1âˆ’Î¸

Thus, for large n and fixed 0 < Î¸ â‰¤ 1, a single scan suffices to achieve randomness.
    A comparison of the results in (6.2) and (6.3) shows a mild advantage for systematic scans.
The effect is most pronounced as Î¸ approaches 1.

6b. Representation theory
     References for the statements in this paragraph are [KSo] and [CR, Â§67C]. The two dimensional
irreducible representations of the Iwahori-Hecke algebra H are indexed by 0 < Î» < n/2 and are
given explicitly by
                                                                                    
            Î»          1/2   âˆ’dÎ¾ (1 + ad)Î¾                   Î»          1/2   a 1 + ad
           Ï (T1 ) = q                        ,    and      Ï (T2 ) = q                  ,   (6.4)
                             Î¾ âˆ’1  âˆ’aÎ¾ âˆ’1                                     1    d

where

  Î¾ = e2Ï€iÎ»/n ,       and the equations a + d = q 1/2 âˆ’ q âˆ’1/2                and      âˆ’ aÎ¾ âˆ’1 + dÎ¾ = q 1/2 âˆ’ q âˆ’1/2

determine a and d. For these representations

                                                      1 (q n âˆ’ 1) (1 âˆ’ Î¾)(1 âˆ’ Î¾ âˆ’1 )
                      cÎ» = 0       and        tÎ» =                                   q(q + 1).                   (6.5)
                                                      n (q âˆ’ 1) (q âˆ’ Î¾)(q âˆ’ Î¾ âˆ’1 )

If n is odd there are two one-dimensional representations of H, the â€œtrivialâ€ representation and
the â€œsignâ€ representation,

                  Ï1 (T1 ) = Ï1 (T2 ) = q,                with        c1 = n,          and        t1 = 1,


               Ïsgn (T1 ) = Ïsgn (T2 ) = âˆ’1,              with       csgn = âˆ’n,        and       tsgn = q n .

If n is even there are two additional one dimensional representations of H,

                                 Ï+ (T1 ) = q,                        Ïâˆ’ (T1 ) = âˆ’1,
                                                            and
                                 Ï+ (T2 ) = âˆ’1,                       Ïâˆ’ (T2 ) = q,

with
                                                                              2q(q n âˆ’ 1)
                             c+ = câˆ’ = 0,             and         t+ = tâˆ’ =               .
                                                                              n(q 2 âˆ’ 1)
26                                  persi diaconis       and     arun ram

    The next Proposition computes the traces on the Iwahori-Hecke algebra which are needed in
order to use Proposition 4.8 to give upper bounds for the convergence of the random and systematic
scan Metropolis walks on the dihedral group.

Proposition 6.6. Let W be the dihedral group of order 2n and let w0 be the longest element of
W . Let Î¸ = q âˆ’1 and TÌƒw = q âˆ’â„“(w)Tw as in Theorem 4.3. If Ï‡Î»H is the character of a two dimensional
representation of the Iwahori-Hecke algebra H then
                              
               Ï‡Î»H (TÌƒw2 0 )2â„“ = 2Î¸ 2â„“n ,   and
                                Î¸ âˆ’ 2 cos(Ï€Î»/n)Î¸ 1/2 âˆ’ 1 2â„“  Î¸ + 2 cos(Ï€Î»/n)Î¸ 1/2 âˆ’ 1 2â„“
     Î»                      2â„“
    Ï‡H ((1/2)(TÌƒ1 + TÌƒ2 ))       =                             +                               ,
                                              2                                2

For the one dimensional irreducible characters of H,
                                                                
             Ï‡1H (TÌƒw2 0 )2â„“ = 1,                  Ï‡sgn
                                                     H
                                                         ( TÌƒ 2 2â„“
                                                             w0 )    = Î¸ 4â„“n ,
                                                                               
             Ï‡1H ((1/2)(TÌƒ1 + TÌƒ2 )2â„“ = 1,         Ï‡sgn
                                                     H
                                                         ((1/2)(  TÌƒ1 + TÌƒ2 )) 2â„“
                                                                                    = Î¸ 2â„“ ,

and, if n is even,
                                                            
                              Ï‡+
                               H
                                  ( TÌƒ 2 2â„“
                                      w0 )    = Ï‡âˆ’
                                                 H
                                                     ( TÌƒ 2 2â„“
                                                         w0 )    = Î¸ 2â„“n ,      and
                                                                             Î¸ âˆ’ 1 2â„“
                 Ï‡+  ((1/2)( TÌƒ1 + TÌƒ2 )) 2â„“
                                               = Ï‡âˆ’
                                                      ((1/2)( TÌƒ1 + TÌƒ2 )) 2â„“
                                                                               =            .
                  H                               H
                                                                                    2


Proof. The formulas for Ï‡Î»H ((TÌƒw2 0 )2â„“ ) follow from Proposition 4.9. The results for the one di-
mensional representations are easy consequences of the definitions of the representations ÏÎ» . Let
us compute Ï‡Î»H ((T1 + T2 )m ) for the two dimensional representations. From the definition of the
representations ÏÎ» in 6.4                        
                              Tr ÏÎ» (T1 + T2 ) = 2q 1/2 (q 1/2 âˆ’ q âˆ’1/2 )
and                                                                                             
                det(ÏÎ» (T1 + T2 )) = q (a âˆ’ dÎ¾)(d âˆ’ aÎ¾ âˆ’1 ) + (1 + Î¾ âˆ’1 )(1 + ad)(1 + Î¾)
                                                                                  
                                   = q (a + d)(âˆ’aÎ¾ âˆ’1 âˆ’ dÎ¾) âˆ’ (1 + Î¾ âˆ’1 )(1 + Î¾)
                                                                             
                                   = q (q 1/2 âˆ’ q âˆ’1/2 )2 âˆ’ (Î¾ 1/2 + Î¾ âˆ’1/2 )2 .

where Î¾ = e2Ï€iÎ»/n . Thus the characteristic polynomial of ÏÎ» (T1 + T2 ) is
                                                                          
 t2 âˆ’2q 1/2 (q 1/2 âˆ’ q âˆ’1/2 )t + q (q 1/2 âˆ’ q âˆ’1/2 )2 âˆ’ (Î¾ 1/2 + Î¾ âˆ’1/2 )2
                                                                                                        
      = t âˆ’ q 1/2 ((q 1/2 âˆ’ q âˆ’1/2 ) + (Î¾ 1/2 + Î¾ âˆ’1/2 )) t âˆ’ q 1/2 ((q 1/2 âˆ’ q âˆ’1/2 ) âˆ’ (Î¾ 1/2 + Î¾ âˆ’1/2 )) .

This determines the eigenvalues of ÏÎ» (T1 + T2 ) and the results follow after substitution of q = Î¸ âˆ’1
and TÌƒi = q âˆ’1 Ti .

    The next two theorems give explicit expressions for the norms kK1â„“ /Ï€ âˆ’ 1k22 for the random
and systematic scan Metropolis walks on the dihedral group. Bounds on rates of convergence based
on these expressions appear in the summary of results in Section 6a above.
                                      Metropolis scans and Hecke algebras                                                                                  27

Theorem 6.7. Let K be the systematic scan Metropolis algorithm on the dihedral group defined
by multiplication by the element TÌƒw2 0 = TÌƒ1 TÌƒ2 TÌƒ1 Â· Â· Â· (2n factors) in the Iwahori-Hecke algebra. Then
                                           2                         
       â„“        2     (4â„“âˆ’1)n     (2â„“âˆ’1)n   Î¸ âˆ’ 1 Î¸n âˆ’ 1
(a) kK1 /Ï€ âˆ’ 1k2 = Î¸          +Î¸                        Â·         âˆ’ 1 âˆ’ Î¸ 2â„“n , and
                                             Î¸âˆ’1 Î¸âˆ’1
    X
(b)      Ï€(x)kKxâ„“ /Ï€ âˆ’ 1k22 = Î¸ 4â„“n + (2n âˆ’ 2)Î¸ 2â„“n .
      xâˆˆW


Proof. From (3.15) we have that
                                                                                                                              
                          X                                                   qn âˆ’ 1                 âˆ’n                 Î¸n âˆ’ 1
                               tÎ» dÎ» = PW (q) = (1 + q)                                      =Î¸           (Î¸ + 1)                    .
                                                                               qâˆ’1                                       Î¸âˆ’1
                          Î»


From Theorem 4.10(a)
                                                                                                 !
                              X                                           X
      kK1â„“ /Ï€         2
                âˆ’ 1k2 =              tÎ» d Î» Î¸   2â„“(â„“(w0)âˆ’cÎ» )
                                                                 =              tÎ» d Î» Î¸   2â„“n
                                                                                                     âˆ’ t1 d1 Î¸ 2â„“n + tsgn dsgn (Î¸ 4â„“n âˆ’ Î¸ 2â„“n )
                              Î»6=1                                         Î»
                               2â„“n                      2â„“n      âˆ’n       4â„“n
                          =Î¸         PW (q) âˆ’ Î¸+ Î¸ (Î¸   âˆ’ Î¸ 2â„“n )
                                                2                
                             (4â„“âˆ’1)n    (2â„“âˆ’1)  Î¸ âˆ’ 1 Î¸n âˆ’ 1
                          =Î¸         +Î¸               Â·        âˆ’ 1 âˆ’ Î¸ 2â„“n .
                                                 Î¸âˆ’1 Î¸âˆ’1
                                          P
Similarly, use the fact that                    Î»âˆˆWÌ‚    d2Î» = 2n to obtain
                                                                                                          !
      X                                         X                                       X
            Ï€(x)kKxâ„“ /Ï€ âˆ’ 1k22 =                       d2Î» Î¸ 2â„“(â„“(w0 )âˆ’cÎ» ) =                 d2Î» Î¸ 2â„“n       âˆ’ d21 Î¸ 2â„“n + d2sgn (Î¸ 4â„“n âˆ’ Î¸ 2â„“n )
      xâˆˆW                                       Î»6=1                                     Î»
                                                 2â„“n            2â„“n           4â„“n        2â„“n
                                         =Î¸            2n âˆ’ Î¸         +Î¸            âˆ’Î¸         = Î¸ 4â„“n + (2n âˆ’ 2)Î¸ 2â„“n .


Theorem 6.8. Let K be the random scan Metropolis algorithm on the dihedral group defined by
multiplication by the element (1/2)(TÌƒ1 + TÌƒ2 ) in the Iwahori-Hecke algebra. Then
                                                                                 
                                    Î¸ 1âˆ’n              Î¸2 âˆ’ 1         Î¸n âˆ’ 1
 kK1â„“ /Ï€         2
            âˆ’ 1k2 = Î¸      2â„“âˆ’n
                                  +
                                       n               Î¸âˆ’1             Î¸âˆ’1
                                                                                                                                                    2â„“
                                                         X           2 âˆ’ 2 cos(2Ï€Î»/n)                             Î¸ + 2 cos(Ï€Î»/n)Î¸ 1/2 âˆ’ 1
                                                  Ã—                                                                                                        ,
                                                                (Î¸ 2 âˆ’ 2 cos(2Ï€Î»/n)Î¸ + 1)                                    2
                                                       0<Î»<n


and
                                                                                                                                2â„“
                     X                                                     X                 Î¸ + 2 cos(Ï€Î»/n)Î¸ 1/2 âˆ’ 1
                           Ï€(x)kKxâ„“ /Ï€                   2
                                                 âˆ’ 1k2 = Î¸      2â„“
                                                                      +             2                                                    .
                                                                                                        2
                     xâˆˆW                                                  0<Î»<n



Proof. Substituting q = Î¸ âˆ’1 in the formulas for tÎ» gives tsgn = Î¸ âˆ’n ,
                                                                                                                                                       
          2                    Î¸n âˆ’ 1                                     Î¸ 1âˆ’n (Î¸ + 1)     2 âˆ’ 2 cos(2Ï€Î»/n)                                     Î¸n âˆ’ 1
 t+ = tâˆ’ = Î¸ 1âˆ’n                                        and          tÎ» =                                                                                     ,
          n                    Î¸2 âˆ’ 1                                           n         2
                                                                                        (Î¸ âˆ’ 2 cos(2Ï€Î»/n)Î¸ + 1)                                   Î¸âˆ’1
28                                        persi diaconis                    and      arun ram

for 0 < Î» < n/2. Thus, if K = (1/2)(TÌƒ1 + TÌƒ2 ), Theorem 4.10a gives
                                                               2â„“                       2â„“
                     X                                    1âˆ’Î¸                        Î¸âˆ’1                            X
  kK1â„“ /Ï€ âˆ’ 1k22 =          tÎ» Ï‡Î» (K 2â„“ ) = t+                          + tâˆ’                     + tsgn Î¸ 2â„“ +              tÎ» Ï‡Î» (K 2â„“ )
                                                           2                          2
                     Î»6=1                                                                                         0<Î»<n/2
                                     n
                                                         2â„“
                             4       Î¸ âˆ’1            Î¸âˆ’1                               X
                = Î¸ 1âˆ’n                                             + Î¸ 2â„“âˆ’n +                   tÎ» Ï‡Î» (K 2â„“ ),
                             n       Î¸2 âˆ’ 1           2
                                                                                     0<Î»<n/2


where the first term appears only if n is even. The result now follows from Proposition 6.6.
Similarly,
                                                                                      2â„“
         X                                    X                                  1âˆ’Î¸                       X
              Ï€(x)kKxâ„“ /Ï€ âˆ’ 1k22 =                   dÎ» Ï‡Î» (K 2â„“ ) = 2                       + Î¸ 2â„“ +               2Ï‡Î» (K 2â„“ )
                                                                                  2
        xâˆˆW                                   Î»6=1                                                      0<Î»<n/2
                                                                                                        2â„“
                                               2â„“
                                                          X             Î¸ + 2 cos(Ï€Î»/n)Î¸ 1/2 âˆ’ 1
                                          =Î¸        +           2                                              .
                                                                                   2
                                                        0<Î»<n




7. The symmetric group
     This section proves Theorem 1.4 and a similar result for a different scanning strategy. The
results show that both scanning strategies require n2 operations up to lead term constants.

Preliminaries
    The symmetric group Sn is generated by the simple transpositions si = (i, i+1), 1 â‰¤ i â‰¤ nâˆ’1,
and the longest element of Sn is the reversal permutation
                                                                                                            
                                 1  2               Â·Â·Â·   nâˆ’1           n                                     n
                     w0 =                                                            with         â„“(w0 ) =      .
                                 n nâˆ’1              Â·Â·Â·    2            1                                     2

The book of Fulton [Fu] provides a review of the representation theory of Sn and we will adopt the
conventions for tableaux used there. The irreducible representations of the Iwahori-Hecke algebra
H are indexed by partitions Î» with n boxes.




                                                      Î»=




Number the rows and columns of Î» as for matrices. If Î»i and Î»â€²j denote the length of the ith row
and j th column of Î» respectively, the content and the hook length of a box b in position (i, j) of Î»
are
                         c(b) = j âˆ’ i    and     hb = Î»i âˆ’ i + Î»â€²j âˆ’ j + 1,
                                    Metropolis scans and Hecke algebras                                               29

respectively. Set
                â„“(Î»)
                X                                             qk âˆ’ 1
       n(Î») =           (i âˆ’ 1)Î»i ,     and let     [k]q =             and [k]q ! = [k]q [k âˆ’ 1]q Â· Â· Â· [2]q [1]q ,
                  i=1
                                                               qâˆ’1
for each positive integer k. With these notations the dimensions dÎ» of the irreducible representa-
tions of H, the generic degrees tÎ» defined in (3.15), and the constants cÎ» defined in Proposition
4.9, are given by
                          n!            X                                [n]q !
                   dÎ» = Y ,        cÎ» =      c(b)    and    tÎ» = q n(Î») Y          ,         (7.1)
                            hb           bâˆˆÎ»                               [h b ]q
                               bâˆˆÎ»                                                       bâˆˆÎ»
respectively (see [Fu, Â§7.2 Prop. 2], [Mac, I Â§7 Ex. 7 and Â§1 Ex. 3], [Hf, 3.4.14] and [Mac, IV
(6.7)]). The dimension dÎ» is also equal to the number of standard tableaux of shape Î», i.e. fillings
of the boxes of Î» with 1, 2, . . . , n such that the rows are increasing left to right and the columns
are increasing top to bottom (see [Fu, p.53]).
     The next lemma provides bounds on the constants in (7.1) which will be useful for proving
bounds on the convergence times of the systematic scan Metropolis walks that we analyze here.
The bounds on cÎ» given in part (c) are essentially those given by Diaconis and Shahshahani, see
[D, 3D Lemma 2].

Lemma 7.2. For each partition Î» let tÎ» , cÎ» and dÎ» be as defined in (7.1).
                                                Î»1    n
     (a)    When Î¸ = 1/q and 0 < Î¸ â‰¤ 1, tÎ» â‰¤ Î¸ ( 2 )âˆ’( 2 ) dÎ» ,
            X                       X          n2j
     (b)      d2Î» = n!,    and           d2Î» â‰¤      ,        for each 1 â‰¤ j â‰¤ n.
                                                j!
            Î»âŠ¢n                      Î»
                                             Î»1 =nâˆ’j
                 ï£± 
                 ï£² Î»1 + 1 (n âˆ’ Î»1 )(n âˆ’ Î»1 âˆ’ 3), if Î»1 â‰¥ n/2,
                 ï£´
                     2     2
     (c)    cÎ» â‰¤
                 ï£´
                 ï£³ 2
                   n /4 âˆ’ n,                     if Î»1 â‰¤ n/2.
Proof. Set Î¸ = 1/q and use [Mac, I Â§1 Ex. 2] and [Mac, III Â§6 Ex. 2], to get
                          n
                                  P      [n]Î¸ !              n     [n]Î¸ !         n X
            tÎ» = Î¸ âˆ’n(Î»)âˆ’( 2 )âˆ’n+( hb ) Y         = Î¸ n(Î» )âˆ’( 2 ) Y         = Î¸ âˆ’( 2 )
                                                         â€²
                                                                                         Î¸ r(Q) ,
                                           [hb ]Î¸                    [hb ]Î¸            Q
                                                  bâˆˆÎ»                     bâˆˆÎ»
where the sum is over all standard tableaux Q of shape Î» and r(Q) is the sum of i such that i + 1 is
to the right
            of iÎ»1in Q. nThus
                                tÎ» is a sum of dÎ» monomials where the lowest degree term has degree
    â€²     n
n(Î» ) âˆ’ 2 â‰¥ 2 âˆ’ 2 . Part (a) follows.
      We can bound   the number of standard tableaux Q of shape Î» with Î»1 = n âˆ’ jâˆš by noting
that there are nj ways of picking the elements not in the first row of Q and at most j! ways of
arranging these to complete a standard tableau. Thus
                                    ï£«           ï£¶2
                                                            2
                        X               X              n p           n2j        n2j
                             d2Î» â‰¤ ï£­         dÎ» ï£¸ â‰¤          j! â‰¤        2
                                                                           j! =     .
                                                       j            (j!)         j!
                         Î»                Î»1 =nâˆ’j
                          Î»1 =nâˆ’j

     The inequalities in (c) are direct consequences of
            cÎ» â‰¤ c(Î»1 ,nâˆ’Î»1 ) , if Î» â‰¥ n/2,             and       cÎ» â‰¤ c(n/2,n/2) , if Î»1 â‰¤ n/2.

Long systematic scan
30                                      persi diaconis              and    arun ram

     As in Section 4a we fix 0 < Î¸ â‰¤ 1 and consider the Markov chain
                                    ï£±
                                    ï£² 1,      if y = si x and â„“(y) > â„“(x),
                        Ki (x, y) = Î¸,        if y = si x and â„“(y) < â„“(x),                              (7.3)
                                    ï£³
                                      1 âˆ’ Î¸, if y = x.

which is produced by applying the Metropolis construction to the base chain
                                             
                                               1, if y = si x,
                                 Pi (x, y) =
                                               0, otherwise,

with the distribution Ï€ as given in (1.1). Recall that the chain Ki can be interpreted as follows:
    From w, try to multiply by si . If this increases the number of inversions of w, carry out
    the multiplication. If it decreases the the number of inversion flip a Î¸-coin and carry out
    the multiplication if the coin comes up heads. Otherwise stay at w.
The long systematic scan Metropolis chain is the chain given by

                     K = (K1 K2 Â· Â· Â· Kn Kn Â· Â· Â· K2 K1 ) Â· Â· Â· (K1 K2 K2 K1 )(K1K1 ).

The following theorem bounds the rate of convergence of this Markov chain. It shows that a single
scan suffices to be close to stationarity.

Proposition 7.4. Let K be the long systematic scan Metropolis walk on the symmetric group
Sn defined by (4.2). Let dÎ» , tÎ» and cÎ» be the constants given in (7.1) Then, for 0 < Î¸ â‰¤ 1,
                     X                n
(a) kK1â„“ /Ï€ âˆ’ 1k22 =      tÎ» dÎ» Î¸ 2â„“(( 2 )âˆ’cÎ» ) ,
                      Î»6=(n)
     and, with â„“ = 1,                               2 n/2         2
                                    kK11 âˆ’ Ï€k2T V â‰¤ en Î¸ âˆ’ 1 + n!Î¸ n /8+5n/4 ,

    which, when Î¸ < 1, approaches 0 as n â†’ âˆž.
    X                        X            n
(b)     Ï€(x)kKxâ„“ /Ï€ âˆ’ 1k22 =    d2Î» Î¸ 2â„“(( 2 )âˆ’cÎ» ) .
     xâˆˆW                             Î»6=(n)
     and, with â„“ = 1,            X                          2 n           2
                                     Ï€(x)kKxâ„“ /Ï€ âˆ’ 1k2T V â‰¤ en Î¸ âˆ’ 1 + n!Î¸ n /2+n ,
                             xâˆˆW

     which, when Î¸ < 1, approaches 0 as n â†’ âˆž.

Proof. By Theorem 4.3 this walk is equivalent to the walk on the Iwahori-Hecke algebra H defined
by multiplication by TÌƒw2 0 with respect to the basis {TÌƒw | w âˆˆ Sn }. Thus the equalities in (a) and
(b) are consequences of Theorem 4.10.
     Fix â„“ = 1. If Î»1 = n âˆ’ j and j â‰¤ n/2 then the bound on cÎ» from Lemma 7.2(c) gives
               Î»1    n       n
             Î¸ ( 2 )âˆ’( 2 )+2( 2 )âˆ’2cÎ» â‰¤ Î¸ j(nâˆ’j/2âˆ’1/2)âˆ’j(jâˆ’3) = Î¸ j(nâˆ’3j/2+5/2) â‰¤ Î¸ j(n/4+5/2) ,

and, by using the bounds in Lemma 7.2(a-b), it follows that
                    n/2                                       n/2 2j
                    X   X                      n              X  n
                                    tÎ» dÎ» Î¸ 2â„“(( 2 )âˆ’cÎ» ) â‰¤
                                                                                         2 n/4
                                                                         Î¸ j(n/4+5/2) â‰¤ en   Î¸
                                                                                                 âˆ’ 1.
                    j=1    Î»6=(n)                             j=1
                                                                    j!
                          Î»1 =nâˆ’j
                           Metropolis scans and Hecke algebras                                          31

When Î»1 â‰¤ n/2, the bound in Lemma 7.2(c) gives
                                X             Î»1     n     n
                                         d2Î» Î¸ ( 2 )âˆ’( 2 )+2( 2 )âˆ’2cÎ» â‰¤ n!Î¸ n
                                                                             2
                                                                                 /8+5n/4
                                                                                            .
                                  Î»
                               Î»1 â‰¤n/2



The upper bound on kK11 âˆ’ Ï€kT V follows by combining these expressions. The upper bound in (b)
is proved similarly.


Short systematic scan
     We now analyze the convergence of the short systematic scan and prove Theorem 1.4 of the
introduction. The short systematic scan Metropolis chain on the symmetric group is given by

                                         K = K1 K2 Â· Â· Â· Kn Kn Â· Â· Â· K2 K1 ,

where Ki is as in (7.3). The Theorem shows that order n short systematic scans are necessary
and suffice to reach stationarity when starting from the identity. In part (bâ€² ) it is shown that for
typical starting values this chain converges in order log n scans.

Theorem 7.5. Let K be the short systematic scan Metropolis algorithm on the symmetric group
defined by (4.2). Let dÎ» , tÎ» and cÎ» be the constants given in (7.1). Then
                      X        X
(a) kK1â„“ /Ï€ âˆ’ 1k22 =        tÎ»   Î¸ 2â„“(nâˆ’1âˆ’c(S(n))), where
                      Î»6=(n)     S

      the sum is over standard tableaux of shape Î» and S(n) denotes the box of S containing n.
  â€²
(a ) For â„“ = n/2 âˆ’ (log n/ log Î¸) + c with c > 0,
                                      2c+1           2
                      kK1â„“ âˆ’ Ï€k2T V â‰¤ eÎ¸    âˆ’ 1 + n!Î¸ n /8âˆ’n(log n)/(log Î¸)+n(c+1/4) .

    Conversely, if â„“ < n/4 then, for fixed 0 < Î¸ < 1, kK1â„“ âˆ’ Ï€kT V tends to 1 as n â†’ âˆž.
    X                         X        X
(b)     Ï€(x)kK1â„“/Ï€ âˆ’ 1k22 =        dÎ»     Î¸ 2â„“(nâˆ’1âˆ’c(S(n))), where
      xâˆˆW                        Î»6=(n)        S

      the sum is over standard tableaux of shape Î» and S(n) denotes the box of S containing n.
  â€²
(b ) For 0 < Î¸ < 1 and â„“ = âˆ’(log n)/(log Î¸) + c with c > 0,
                                                                                     n
                         X                                
                                                               Î¸ 2c
                                                                                Î¸c             âˆš
                               Ï€(x)kKxâ„“ /Ï€           2
                                               âˆ’ 1k2 â‰¤ e              âˆ’1 +                 e1/12 2Ï€n.
                                                                                 e
                        xâˆˆSn




Proof. By Theorem 4.3 the Markov chain K is the random walk on {TÌƒw | w âˆˆ Sn } defined by
multiplication by TÌƒnâˆ’1 Â· Â· Â· TÌƒ2 TÌƒ12 TÌƒ2 Â· Â· Â· TÌƒnâˆ’1 in the Iwahori-Hecke algebra H corresponding to the
symmetric group Sn . Let H â€² be the Iwahori-Hecke algebra corresponding to the symmetric group
Snâˆ’1 and let H be the Iwahori-Hecke algebra corresponding to Sn . Let w0â€² be the longest element
of Snâˆ’1 and let w0 be the longest element of Sn . The inclusion Snâˆ’1 âŠ† Sn induces an inclusion
32                                          persi diaconis            and       arun ram

H â€² âŠ† H of the corresponding Iwahori Hecke algebras. In H the generators Ti are invertible with
Tiâˆ’1 = q âˆ’1 Ti + (1 âˆ’ q âˆ’1 ). Then

                                           TÌƒw2 0 TÌƒwâˆ’2                     2
                                                      â€² = TÌƒnâˆ’1 Â· Â· Â· TÌƒ2 TÌƒ1 TÌƒ2 Â· Â· Â· TÌƒnâˆ’1
                                                        0


and so it follows from Proposition 4.9 that, in the representation ÏÎ» of H indexed by the partition
Î», the element

                      K = TÌƒnâˆ’1 Â· Â· Â· TÌƒ2 TÌƒ12 TÌƒ2 Â· Â· Â· TÌƒnâˆ’1       has eigenvalues                Î¸ nâˆ’1âˆ’c(S(n)) ,

where S runs over standard tableaux of shape Î» and S(n) denotes the box of S containing n. This
determines the eigenvalues of K 2â„“ in the representation ÏÎ» and so
                                               X
                                 Ï‡Î»H (K 2â„“ ) =   Î¸ 2â„“(nâˆ’1âˆ’c(S(n))),
                                                                 S

where Ï‡Î»H is the irreducible character of the Iwahori-Hecke algebra corresponding to the partition
Î». Parts (a) and (b) now follow from Proposition 4.8.
    Using Î¸ 2â„“(nâˆ’1âˆ’c(S(n))) â‰¤ Î¸ 2â„“(nâˆ’Î»1 ) , and the bound for tÎ» in Lemma 7.2 gives
                          X Î»1 n                X               X 1
        kK1â„“ /Ï€ âˆ’ 1k22 â‰¤       Î¸ ( 2 )âˆ’( 2 ) dÎ»   Î¸ 2â„“(nâˆ’Î»1 ) â‰¤    Î¸ 2 (nâˆ’Î»1 )(nâˆ’Î»1 +4â„“+1âˆ’2n) d2Î» ,
                                 Î»6=(n)                     S                   Î»6=(n)


since dÎ» is the number of standard tableaux of shape Î». Fix â„“ = n/2 âˆ’ (log n)/(log Î¸) + c. Then,
using the bound on the sum of d2Î» from Lemma 7.2,

     n/2                                          n/2 1 j(1+4â„“+1âˆ’2n+4(log n/ log Î¸))                âˆž
     X   X            1
                                                  X  Î¸2                                             X Î¸ (2c+1)j          2c+1
                     Î¸ 2 j(j+4â„“+1âˆ’2n) d2Î» â‰¤                                                     =                 = eÎ¸          âˆ’ 1.
     j=1    Î»6=(n)                                j=1
                                                                       j!                           j=1
                                                                                                           j!
           Î»1 =nâˆ’j



The function (n âˆ’ Î»1 )(n âˆ’ Î»1 + 4â„“ + 1 âˆ’ 2n) has a minimum at n âˆ’ Î»1 = (âˆ’1/2)(4â„“ + 1 âˆ’ 2n). At
this minimum Î»1 = n âˆ’ 2(log n)/(log Î¸) + 2c + 1/2 â‰¥ n/2, and so
     X 1                                    1                              2
         Î¸ 2 (nâˆ’Î»1 )(nâˆ’Î»1 +4â„“+1âˆ’2n) d2Î» â‰¤ Î¸ 2 (n/2)(n/2+4â„“+1âˆ’2n) n! = n!Î¸ n /8âˆ’(n log n)/(log Î¸)+n(c+1/4) .
        Î»
     Î»1 â‰¤n/2



Combining these sums establishes the upper bound in (aâ€² ).
   To prove the lower bound in (aâ€² ) let
                                                                             
                                                          n 
     A = {w âˆˆ Sn | â„“(w) > 2â„“(n âˆ’ 1)} = w âˆˆ Sn  â„“(w) âˆ’        < 2(n âˆ’ 1)(n/4 âˆ’ â„“) .
                                                            2

Since each pass of the systematic scan can change the length of a permutation by at most 2(n âˆ’ 1),

                                                            K1â„“ (A) = 0.                                                           (7.6)

From equation (2.13),
                                              n                  n                        n                  
                         (n âˆ’ 1) X j           X       (n âˆ’ 1) X jÎ¸ j                                        n
           EÏ€ (â„“(w)) = âˆ’        +            =     j âˆ’        +            =                                    + O(n),
                          1âˆ’Î¸     j=2
                                      1 âˆ’ Î¸j   j=2
                                                        1âˆ’Î¸     j=2
                                                                    1 âˆ’ Î¸j                                   2
                                    Metropolis scans and Hecke algebras                                                        33

  and
                                                                         n
                                                  (n âˆ’ 1)Î¸ X j 2 Î¸ j           (n âˆ’ 1)Î¸
                          VarÏ€ (â„“(w)) =                   2
                                                            âˆ’           j  2
                                                                             =          + O(1).
                                                  (1 âˆ’ Î¸)     j=2
                                                                  (1 âˆ’ Î¸ )     (1 âˆ’ Î¸)2

  Thus Chebychevâ€™s inequality implies that, when n is large,
                                                                                   
                               VarÏ€ (â„“(w))                            Î¸
            Ï€(A) âˆ¼ 1 âˆ’                          âˆ¼ 1âˆ’                                    .                                    (7.7)
                         (2(n âˆ’ 1)(n/4 âˆ’ â„“))2             4(n âˆ’ 1)(1 âˆ’ Î¸)2 (n/4 âˆ’ â„“)2

  If â„“ < n/4 then the right hand side approaches 1 as n â†’ âˆž. Thus (7.5) and (7.6) imply that
  kK1â„“ âˆ’ Ï€kT V â†’ 1 as n â†’ âˆž and this proves the second statement of (aâ€² ).
        (bâ€² ) Using the bound Î¸ 2â„“(nâˆ’1âˆ’c(S(n))) â‰¤ Î¸ 2â„“(nâˆ’Î»1 ) , gives
                                       X                                               X
                                                 Ï€(x)kK1â„“ /Ï€ âˆ’ 1k22 â‰¤                        Î¸ 2â„“(nâˆ’Î»1 ) d2Î» .
                                       xâˆˆSn                                         Î»6=(n)


  Fix â„“ = âˆ’(log n)/(log Î¸) + c. Using the bound in Lemma 7.2(a) gives

                    n/2                              n/2               2j        n/2 2j(â„“+(log n/ log Î¸))
                                                               2â„“j n                Î¸
                    X   X                            X                           X                                  2c
                                     Î¸ 2â„“j d2Î»   â‰¤         Î¸                 =                                   = eÎ¸ âˆ’ 1,
                    j=1    Î»6=(n)                    j=1
                                                                    j!           j=1
                                                                                                 j!
                          Î»1 =nâˆ’j



  and, by using the bound in Lemma 7.2(b) and the bound on n! given in [Fe, (9.15)],
                                                                                                       n
                           X                                                                       Î¸c             âˆš
                                    Î¸ 2â„“(nâˆ’Î»1 ) d2Î»            â„“n
                                                      â‰¤ Î¸ n! = n!n                âˆ’n cn
                                                                                       Î¸   â‰¤                 e1/12 2Ï€n.
                             Î»
                                                                                                   e
                          Î»1 â‰¤n/2



  The result follows by combining the bounds for these two sums.



                                                                 References

[Am1] Y. Amit, Convergence properties of the Gibbs sampler for perturbations of Gaussians, Ann.
      Statistics 24 (1995), 122â€“140.
[Am2] Y. Amit, On rates of convergence of stochastic relaxation for Gaussian and non-Gaussian
      distributions, J. Multiv. Anal. 38 (1991), 82â€“99.
 [AG] Y. Amit and U. Grenander, Comparing sweep strategies for stochastic relaxation, J. Mul-
      tiv. Anal. 37 (1991), 197â€“222.
 [AK] S. Ariki and K. Koike, A Hecke algebra of (Z/rZ) â‰€ Sn and construction of its irreducible
      representations, Adv. Math. 106 (1994), 216â€“243.
  [Be] E. Belsley, Rates of convergence of random walk on distance regular graphs, Probab. Theory
       Related Fields 112 (1998), 493â€“533.
[Be2] E. Belsley, Rates of convergence of Markov chains related to association schemes, Ph.D.
      thesis, Harvard, 1993.
  34                              persi diaconis     and    arun ram

 [BF] P. Barone and A. Frigessi, Improving stochastic relaxation for Gaussian random fields,
      Probab. Eng. Inform. Sci. 4 (1990), 369â€“389.
[BHV] L. Billera, S. Holmes, K. Vogtman, Geometry of the space of phylogenetic trees, Technical
      Report, Department of Statistics, Stanford University, 1999.
  [BS] E. Brieskorn and K. Saito, Artin-Gruppen und Coxeter-Gruppen, Invent. Math. 17 (1972),
       245â€“271.
[Bou] N. Bourbaki, Groupes et algeÌ€bres de Lie, Ch. 4,5 et 6, Masson, Paris, 1981.
 [Bw] K. Brown, Buildings, Springer-Verlag, New York-Berlin, 1989.
  [Ca] R. Carter, Finite groups of Lie type â€“ Conjugacy classes and complex characters, Wiley-
       Interscience, John Wiley & Sons, Inc., New York, 1985.
  [Cr] D. Critchlow, Metric methods for analyzing partially ranked data, Lecture Notes in Statis-
       tics 34, Springer-Verlag, Berlin, 1985.
 [CR] C. Curtis and I. Reiner, Methods of representation theory â€“ with applications to finite
      groups and orders, Volumes I and II, Wiley-Interscience, John Wiley & Sons, Inc., New York,
      1981 and 1987.
  [De] P. Deligne, Les immeubles des groupes de tresses geÌneÌraliseÌs, Invent. Math. 17 (1972), 273â€“
       302.
   [D] P. Diaconis, Group representations in probability and statistics, Institute of Mathematical
       Statistics, Hayward, CA, 1988.
 [DH] P. Diaconis and P. Hanlon, Eigen-analysis for some examples of the Metropolis algorithm,
      in Hypergeometric functions on domains of positivity, Jack polynomials, and applications
      (Tampa, FL, 1991), 99â€“117, Contemp. Math., 138, Amer. Math. Soc., Providence, RI, 1992.
 [Det] P. Diaconis, S. Holmes, S. Janson, S. Lalley, R. Pemantle, Metrics on compositions
       and coincidences among renewal sequences, in Random discrete structures, D. Aldous and R.
       Pemantle eds., Springer, New York, 1996.
 [DS] P. Diaconis and L. Saloff-Coste, What do we know about the Metropolis algorithm?, J.
      Computer and System Sciences 57 (1998), 20-36.
  [Fe] W. Feller, An introduction to probability theory and its applications, J. Wiley and Sons,
       New York, 1968.
  [Fi] G. Fishman, Coordinate selection rules for Gibbs sampling, Ann. Appl. Probab. 6 (1996)
       444â€“465.
 [FV] M. Figner and J. Verducci, Probability models and statistical analyses for ranking data,
      Springer Lecture Notes in Statistics 80, Springer, New York, 1993.
  [Fu] W. Fulton, Young tableaux, With applications to representation theory and geometry, Lon-
       don Math. Soc. Student Texts 35, Cambridge Univ. Press, Cambridge, 1997.
 [GS] J. Goodman and A. Sokal, Multigrid Monte-Carlo method. Conceptual foundations, Phys.
      Rev. D 40 (1989), 2035â€“2071.
 [HH] J. Hammersley and D. Handscomb, Monte Carlo Methods, Chapman and Hall, London,
      1964.
                           Metropolis scans and Hecke algebras                                   35

 [Hf] P.N. Hoefsmit, Representations of Hecke algebras of finite groups with BN-pairs of classical
      type, Ph.D. Thesis, University of British Columbia, 1974.
 [Hu] J. Humphreys, Reflection groups and Coxeter groups, Cambridge University Press, 1990.
 [KS] J. Kemeny and L. Snell, Finite Markov chains, The University Series in Undergraduate
      Mathematics, Van Nostrand, Princeton, 1960.
[KSo] R. Kilmoyer and L. Solomon, On the theorem of Feit-Higman, J. Combinatorial Theory
      A 15 (1973), 310-322.
[Le1] G. Letac, ProbleÌ€mes classiques de probabiliteÌ sur un couple de Gelfand, in Lect. Notes in
      Math. 861, Springer-Verlag, New York, 1981.
[Le2] G. Letac, Les fonctions spheÌriques dâ€™un couple de Gelfand symeÌtrique et les chaiÌ‚nes de
      Markov, Advances in Appl. Prob. 14 (1982), 272-294.
[Ma] C.L. Mallows, Non-null ranking models I, Biometrika 44 (1957), 114-130.
[Mac] I.G. Macdonald, Symmetric functions and Hall polynomials, Second edition, Oxford Math-
      ematical Monographs, Oxford University Press, New York, 1995.
[Mar] J. Marden, Analyzing and modeling rank data, Monographs on Statistics and Applied Prob-
      ability 64, Chapman & Hall, London, 1995.
[MR] N. Metropolis, A. Rosenbluth, M. Rosenbluth, A. Teller and E. Teller, Equa-
     tions of state calculations by fast computing machines, J. Chem. Phys. 21 (1953), 1087-1092.
 [Pa] I. Pak, When and how n choose k, in Randomization methods in algorithm design (Princeton,
      NJ, 1997), 191â€“238, DIMACS Ser. Discrete Math. Theoret. Comput. Sci. 43, Amer. Math.
      Soc., Providence, RI, 1999.
 [Ra] A. Ram, Seminormal representations of Weyl groups and Iwahori-Hecke algebras, Proc. Lon-
      don Math. Soc. (3) 75 (1997), 99â€“133.
 [RS] G. Roberts and S. Sahk, Updating schemes, correlation schemes, correlation structure,
      blocking and parameterization for the Gibb sampler, J. Royal Statist. Soc. 59 (1997), 291â€“318.
[RX] K. Ross and D. Xu, Hypergroup deformations and Markov chains, J. Theor. Probability 7
     (1994), 813â€“830.
 [SB] W. Shannon and D. Banks, Combining classification trees using MLE, Statistics in Medicine
      18 (1999), 727-740.
 [SC] L. Saloff-Coste, Lectures on probability theory and statistics (Saint-Flour, 1996), 301â€“413,
      Lecture Notes in Math. 1665, Springer, Berlin, 1997.
 [Sc] C. Schoolfield, Random walks on wreath products of groups and Markov chains on related
      homogeneous spaces, Ph.D. Thesis, Dept. of Appl. Math., Johns Hopkins Univ., 1998.
  [Si] J. Silver, Weighted PoincareÌ and exhaustive approximation techniques for scaled Metropolis-
       Hastings and spectral total variation convergence bounds in infinite commutable Markov chain
       theory, Ph.D. Thesis, Dept. of Math., Harvard University, 1996.
